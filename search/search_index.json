{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Science IT Technical Documentation","text":""},{"location":"#top-pages","title":"Top Pages","text":"<ul> <li> Getting Started with Lawrencium </li> <li> Open OnDemand Portal </li> <li>MyLRC User Account Portal </li> <li>LRC Slurm Jobscript Generator </li> </ul>"},{"location":"#systems-status","title":"Systems Status","text":"<ul> <li>HPC Service Announcements </li> <li>HPC Clusters Live Status - Warewulf Overview </li> </ul>"},{"location":"#hpc-helpdesk","title":"HPC Helpdesk","text":"<ul> <li> HPC Email Support: hpcshelp@lbl.gov (Creates a ticket in AskUS)</li> <li> AskUS Request Form: HPC Help Request</li> <li>Office Hours: HPC &amp; Science IT Office Hours: Every Wednesday 10:30-Noon (Zoom)</li> </ul>"},{"location":"#science-it-consulting","title":"Science IT Consulting","text":"<ul> <li>Schedule a Science IT Consulting Engagement</li> <li>Email Science IT: scienceit@lbl.gov </li> <li>Office Hours: HPC &amp; Science IT Office Hours: Every Wednesday 10:30-Noon (Zoom)</li> </ul>"},{"location":"#other-resources","title":"Other Resources","text":"<ul> <li> Science IT GitHub Repository</li> <li>LBNL IT Division Homepage </li> <li>LBNL Library E-Book Collection </li> <li>Mac &amp; PC Support Support </li> <li>NERSC Technical Documentation </li> <li>UC Berkeley Research IT Documentation </li> </ul>"},{"location":"#cant-find-what-you-are-looking-for","title":"Can't find what you are looking for?","text":"<ul> <li> <p>Ask a LLM Model in the CBorg AI Portal .</p> </li> <li> <p>Contact us at scienceit@lbl.gov to discuss your needs in scientific computing, research data management, cloud computing, AI/ML workflows, and more.</p> </li> </ul>"},{"location":"construction/","title":"Under Construction","text":"<p>This page is a placeholder.</p>"},{"location":"help/","title":"Support &amp; Training Resources","text":""},{"location":"help/#hpc-helpdesk","title":"HPC Helpdesk","text":"<p>Contact us for help with debugging jobs on our HPC clusters, software installation, user account management and more.</p> <ul> <li>HPC Email Support: hpcshelp@lbl.gov (Creates a ticket in AskUS)</li> <li>AskUS Request Form: HPC Help Request</li> <li>Office Hours: HPC &amp; Science IT Office Hours: Every Wednesday 10:30-Noon (Zoom)</li> </ul>"},{"location":"help/#science-it-consulting","title":"Science IT Consulting","text":"<p>Science IT Consultants bring broad expertise in technology deployment, high performance computing and cloud infrastructure. Contact us to discuss your needs in scientific computing, research data management, cloud computing, AI/ML workflows, and more.</p> <ul> <li>Schedule a Science IT Consulting Engagement</li> <li>Email Science IT: scienceit@lbl.gov </li> <li>Office Hours: HPC &amp; Science IT Office Hours: Every Wednesday 10:30-Noon (Zoom)</li> </ul>"},{"location":"help/#hpc-training-and-workshops","title":"HPC Training and Workshops","text":"<p>Click here  for a list of previous HPC trainings and workshops.</p>"},{"location":"help/#upcoming-zoomin_person-trainings","title":"Upcoming Zoom/In_person Trainings","text":"<p>HPCS team provides training in the beginning of the year and in fall to introduce new users to the basics of HPC at Lawrencium. Upcoming trainings are announced here. </p>"},{"location":"help/#lrc-101-spring-training","title":"LRC 101 Spring Training","text":"<p>Date: Monday, April 7th, 2025 Time: 10:30 AM to 12 PM Training slides and recording. </p>"},{"location":"help/#fast-trackhpc-training-videos","title":"Fast track\u2014HPC training videos","text":"<p>In this section we will be publishing short videos to learn HPC at lightning speed. </p>"},{"location":"cloud/","title":"Cloud Services","text":"<p>The IT Division encourages the use of cloud computing for science as an effective way to to meet specific research and computing needs. Many researchers have found that cloud computing works well for quickly bringing up computing infrastructure and that it scales well for bursty workloads such as data releases. Others have found that cloud tools for machine learning, search and workflow orchestration are very effective.</p> <p>To facilitate cloud use at Berkeley Lab, Science IT has secured master payer contracts with Amazon Web Services and Google Cloud Platform. These master payer agreements provide LBNL scientists, staff, and other affiliated researchers with access to the full suite of AWS and GCP services without use of a purchase order or need to create a contract vehicle.  This is considered a self-managed cloud offering as you are expected to manage your own usage of services in your chosen cloud environment.</p> <p>Some of the benefits of using cloud computing services through the Lab\u2019s Master Payer agreement are:</p> <ul> <li>LBNL and DOE have negotiated better pricing and significant discounts for both AWS and GCP as compared to regular commercial list pricing.</li> <li>Cost savings and discounts are automatically applied as part of the program.</li> <li>There are no fees or charges for having an account on either service You only pay for the services and resources that are actually in use on either service</li> <li>An 11% overhead fee is charged (procurement pass-through burden)</li> <li>Charges are applied monthly via recharge using a standard Project ID (PID)</li> <li>You have access to Account Management, Cloud Solution Architects, and Subject Matter experts through the Science IT Consultants, who can assist with how to use cloud services, optimize costs, and provide technical guidance on best practices, but do not provide an ongoing service model.</li> <li>Science IT works with AWS and GCP provides various trainings throughout the year and also works to provide access to experts for consultations</li> </ul>"},{"location":"cloud/#accessing-cloud-services","title":"Accessing Cloud Services","text":"<p>The process for getting a cloud account (AWS) or project (Google Cloud) are different.  Users requesting an AWS account will need to work with the ScienceIT cloud team to get an account, and users needing a Google Cloud project can create their own projects after completing an initial 30-45 minute discussion with the ScienceIT cloud team.</p>"},{"location":"cloud/aws/","title":"Requesting an AWS Account at LBNL","text":"<p>This document outlines the process for LBNL users to request and access an Amazon Web Services (AWS) account.</p> <p>Important: LBNL users cannot self-register AWS accounts using their LBL identity. All AWS account requests must be submitted through the Science IT cloud team.</p>"},{"location":"cloud/aws/#prerequisites","title":"Prerequisites","text":"<ul> <li>You must be a LBNL staff member to get a LBNL AWS account.  Interns and external collaborators are not eligible for an AWS account through LBNL, however a LBNL staff member can be the point of contact and can provide access to an AWS account through IAM logins to interns and external collaborators</li> </ul>"},{"location":"cloud/aws/#requesting-an-aws-account","title":"Requesting an AWS Account","text":"<ol> <li>Email the Science IT Cloud Team:<ul> <li>Send an email to <code>scienceit@lbl.gov</code> requesting an AWS account.</li> </ul> </li> <li>Provide Necessary Information:<ul> <li>In your email, include the following information:<ul> <li>Indicate if you or someone else will be the \"owner\" and contact for the account.  This information is needed by both the LBNL cloud team and Cybersecurity.</li> <li>A Project ID for recharges.</li> </ul> </li> </ul> </li> <li>Account Creation:<ul> <li>The Science IT cloud team will setup a time to meet with the account owner to create and configure the AWS account.  </li> <li>Once the account is created, the setup and enabling of MFA is required before the account can be used.</li> </ul> </li> </ol>"},{"location":"cloud/aws/#enabling-multi-factor-authentication-mfa","title":"Enabling Multi-Factor Authentication (MFA)","text":"<p>After your AWS account is created, at that time you must enable Multi-Factor Authentication (MFA) to access and use any AWS services and resources in the account. This is a security requirement for all LBNL AWS accounts.</p> <p>For MFA, you can use a physical hardware token, an Authenticator app, or a Passkey.</p> <ol> <li>Log in to the AWS Management Console:<ul> <li>Use the credentials provided by the Science IT cloud team to log in to the AWS Management Console: aws.amazon.com/console.</li> </ul> </li> <li>Navigate to Security Credentials:<ul> <li>Select the drop down indicated by the account name in the upper right corner of the page.</li> <li>Click the \"Security credentials\" tab.</li> </ul> </li> <li>Assign MFA device:<ul> <li>In the \"Multi-factor authentication (MFA)\" section, click \"Assign MFA device\".</li> </ul> </li> <li>Choose MFA device type - Authenticator app option:<ul> <li>Select \"Virtual MFA device\" and click \"Continue\".</li> </ul> </li> <li>Install an Authenticator App:<ul> <li>You will need to install an authenticator app on your smartphone or computer. Popular options include:<ul> <li>Google Authenticator</li> <li>Authy</li> <li>Microsoft Authenticator</li> </ul> </li> <li>Scan the QR code displayed on the AWS screen with your authenticator app, or manually enter the secret key.</li> </ul> </li> <li>Enter MFA Codes:<ul> <li>Enter the two consecutive MFA codes generated by your authenticator app into the AWS console and click \"Assign MFA\".</li> </ul> </li> <li>Cleanup:<ul> <li>You will need to log out of the account, and then log back in using MFA in order for AWS services and resources to become available.</li> </ul> </li> </ol>"},{"location":"cloud/aws/#important-notes","title":"Important Notes","text":"<ul> <li>Account Management: The Science IT cloud team manages all LBNL AWS accounts.</li> <li>Security: MFA is mandatory for the root user on all LBNL AWS accounts.</li> <li>Support: For any questions or assistance, contact the Science IT cloud team at <code>scienceit@lbl.gov</code>.</li> </ul>"},{"location":"cloud/gcp/","title":"Creating a Google Cloud Project","text":"<p>This document outlines the steps for users to create a LBNL Google Cloud users to create a new Google Cloud project, ensuring proper billing, organization, and location settings.</p>"},{"location":"cloud/gcp/#prerequisites","title":"Prerequisites","text":"<ul> <li>You must use your LBNL email identity to create a Google Cloud project within the <code>lbl.gov</code> organization.</li> <li>While any LBNL user can create a Google Cloud, you must have access to the \"LBNL\" Billing Account in order to create and run resources and services within the account.</li> <li>If you create a Google Cloud project without selecting a Billing Account, you won't be able to create any resources in your project and instead you'll be prompted to attach your project to a Billing Account. </li> <li>Access to the \"LBNL\" Billing Account is provided to users upon request and after a short discussion with the LBNL Cloud Team about the parameters and rules of using Google Cloud.</li> </ul>"},{"location":"cloud/gcp/#step-by-step-instructions","title":"Step-by-Step Instructions","text":"<ol> <li> <p>Navigate to the Google Cloud Console:</p> <ul> <li>Ensure you are logged into Google with your LBNL account.</li> <li>Open your web browser and go to the Google Cloud Console: console.cloud.google.com.</li> </ul> </li> <li> <p>Create a New Project:</p> <ul> <li>Click on the project selector dropdown at the upper left area on the top of the page (it usually displays \"Select a project\" or has the name of another existing project displayed).</li> <li>Click the \"New Project\" button.</li> </ul> </li> <li> <p>Project Details:</p> <ul> <li>Project Name: Enter a descriptive name for your project. Choose a name that clearly identifies the purpose of the project.</li> <li>Billing Account:<ul> <li>Click anywhere in the Billing Account box to open up the drop down list.</li> <li>Select the \"LBNL\" billing account. This ensures your project's costs are billed directly to LBNL for recharge against your Project ID.</li> </ul> </li> <li>Organization:<ul> <li>Ensure the \"lbl.gov\" organization is displayed in the list.  As long as you're logged in with your LBNL account this will be filled in for you automatically.  This ensures your project is associated with LBNL's Google Cloud organization.</li> </ul> </li> <li>Location:<ul> <li>Click the \"Browse\" button.</li> <li>Navigate down one level from the root \"lbl.gov\" organization, and select the folder named \"Common\". This is where most standard LBNL projects should be created.</li> </ul> </li> </ul> </li> <li> <p>Create the Project:</p> <ul> <li>Review the project details to ensure they are correct.</li> <li>Click the \"Create\" button.</li> </ul> </li> <li> <p>Project Creation and Selection:</p> <ul> <li>Google Cloud will begin creating your project. This process may take a few moments.</li> <li>Once the project is created, you will be automatically redirected to the project's dashboard. If not, you can select your newly created project from the project selector dropdown.</li> </ul> </li> </ol>"},{"location":"cloud/gcp/#screenshots","title":"Screenshots","text":"<ul> <li>Initial View: </li> <li>\"Browse\" Location and select \"Common\" folder: </li> <li>Completed with the location set to \"Common\" </li> </ul>"},{"location":"cloud/gcp/#important-notes","title":"Important Notes","text":"<ul> <li>Project Naming Conventions: Use a meaningful name to clearly indicate the purpose of the project to maintain consistency.</li> <li>Billing Account: Using the correct \"LBNL\" billing account is crucial for proper cost tracking and management.</li> <li>Organization and Location: Selecting the \"lbl.gov\" organization and the \"Common\" folder ensures your project adheres to LBNL's organizational structure and resource management policies.</li> <li>Permissions: If you encounter issues creating a project, contact ScienceIT@lbl.gov to verify your permissions and access.</li> </ul>"},{"location":"data/globus-aws-s3-connector/","title":"Using the Globus AWS S3 Connector","text":"<p>In order to setup Globus to access a S3 bucket, you'll need to have an IAM access key ID and a secret key ready to go. Due to Globus's implementation of the connector you can only add a single IAM access key ID and secret key to your Globus configuration, however you'll have access to any buckets that IAM access key ID is configured to have access to.</p> <p>Please note that for your first time setting up the S3 connector you\u2019ll have to go through various \u201cconsent\u201d and \u201cauthorization\u201d prompts, and those steps are not documented here.  Giving consent is a standard part of the Globus process whereby you authorize Globus to perform additional privileged operations with the selected endpoint.  If you\u2019ve already given permissions to Globus for the S3 connector, you might not see the consent steps.</p> <p>Note</p> <p>This guide assumes you\u2019ve already setup a S3 bucket and configured the IAM access permissions to that bucket. If you need help doing that, see the AWS S3 documentation  for more information.</p>"},{"location":"data/globus-aws-s3-connector/#select-globus-s3-endpoint","title":"Select Globus S3 Endpoint","text":"<ul> <li>Login with Globus at https://globus.lbl.gov.</li> <li>Select \"Endpoints\" from the left navigation.</li> <li>Enter \"LBNL AWS S3 Collection\" into the search textbox.</li> <li>From the list of endpoints that appear, click on the \"LBNL AWS S3 Collection\" link.</li> </ul>"},{"location":"data/globus-aws-s3-connector/#setup-credentials","title":"Setup Credentials","text":"<ul> <li>Click on the \"Credentials\" tab of the \"LBNL AWS S3 Collection\" endpoint page.</li> <li>Here is where you register your AWS IAM access key ID and secret key with Globus.</li> </ul> <ul> <li>After you've entered them, click the \"Continue\" button, and you'll be taken back to the full \"Credentials\" tab where you can see your saved AWS access credentials.</li> </ul> <ul> <li>At this point you are set up to access the S3 buckets with Globus. Click the \"Overview\" tab, and then the \"Open in File Manager\" button to see the S3 buckets and the data that are available using your AWS credentials.</li> </ul>"},{"location":"data/globus-aws-s3-connector/#create-guest-collection-from-lbnl-aws-s3-collection","title":"Create Guest Collection from LBNL AWS S3 Collection","text":"<ul> <li>Click on the \"Collections\" tab of the \"LBNL AWS S3 Collection\" endpoint, and then click on the \"Add a Guest Collection\".</li> </ul> <ul> <li> <p>Enter in the path to the top level folder you want visible in your collection in the \u201cDirectory\u201d field (or enter \u201c/\u201d to use all buckets available to those credentials in your collection).  You can also click the \u201cBrowse\u201d button to get a directory view and select the bucket or subfolder folder you want.</p> </li> <li> <p>Enter a value for the \u201cDisplay Name\u201d field, and then click the \u201cCreate Collection\u201d button.</p> </li> </ul> <p></p>"},{"location":"data/globus-google-cloud-storage-connector/","title":"Using the Globus Google Cloud Storage Connector","text":"<p>These are the instructions for configuring Globus to access a Google Cloud Storage bucket using your regular LBL account credentials. For this connector, unlike the S3 connector, you don\u2019t use an access token + secret key or a service account setup.  You\u2019ll authorize the Google Cloud Storage connector to use your LBL account and from that you\u2019ll have access to any buckets that your LBL account has access to.</p> <p>Please note that for your first time setting up the Google Cloud Storage connector you\u2019ll have to go through various \u201cconsent\u201d and \u201cauthorization\u201d prompts, and those steps are not documented here.  Giving consent is a standard part of the Globus process whereby you authorize Globus to perform additional privileged operations with the selected endpoint.  If you\u2019ve already given permissions to Globus for the Google Cloud Storage connector, you might not see the consent steps.</p>"},{"location":"data/globus-google-cloud-storage-connector/#select-globus-google-cloud-storage-endpoint","title":"Select Globus Google Cloud Storage Endpoint","text":"<ul> <li>Login with Globus at https://globus.lbl.gov</li> <li>Select \"Endpoints\" from the left navigation.</li> <li>Enter \"LBNL Google Cloud\" into the search textbox.</li> <li>From the list of endpoints that appear, click on the \"LBNL Google Cloud Storage Collection\" link.</li> </ul>"},{"location":"data/globus-google-cloud-storage-connector/#setup-credentials","title":"Setup Credentials","text":"<ul> <li>Click on the \"Credentials\" tab of the \"LBNL Google Cloud Storage Collection\" endpoint page.</li> <li>Here you authenticate your LBL credentials for the connector.</li> </ul> <ul> <li>After you've authenticated your account, click the \"Continue\" button, and you'll be taken back to the full \"Credentials\" tab where you can see your active LBL account credentials.</li> </ul> <p>Note</p> <p>One important thing to note is that due to the required Globus configuration for the \u201cLBNL Google Cloud Storage Collection\u201d you will be unable to view the \u201cFile Manager\u201d from the root of the main collection (you\u2019ll see an error message if you try) so you must use a Guest Collection to view files in your buckets.  At this point you are authenticated and ready to add one or more Guest Collections to access Google Cloud Storage buckets with Globus. </p>"},{"location":"data/globus-google-cloud-storage-connector/#create-guest-collection-from-google-cloud-storage-connector","title":"Create Guest Collection from Google Cloud Storage Connector","text":"<ul> <li>Click on the \"Collections\" tab of the \"LBNL Google Cloud Storage Connector\" endpoint, and then click on the \"Add a Guest Collection\".</li> </ul> <ul> <li>Enter in the name of the bucket and any pathing within that bucket that you want to use as the top-level folder of your collection in the \u201cDirectory\u201d field.  Due to the required Globus configuration of the \u201cLBNL Google Cloud Storage Collection\u201d you will be unable to \u201cBrowse\u201d the directory, so you must enter an existing bucket name in the directory field.  Enter any value for the \u201cDisplay Name\u201d field, and then click the \u201cCreate Collection\u201d button.</li> </ul>"},{"location":"data/globus-google-drive/","title":"Globus for Google Drive","text":"<ul> <li>Search for LBNL Gdrive Access (or use the link just above) and click on the endpoint.</li> </ul>"},{"location":"data/globus-google-drive/#creating-a-guest-collection-in-gdrive","title":"Creating a Guest Collection in Gdrive","text":"<ul> <li>Click on the \"Collections\" tab and authenticate if prompted by hitting the \"Continue\" button. Consent to allow Globus to manage the collection</li> <li>Click Add a Guest Collection</li> </ul> <ul> <li>Enter the drive path you want to view and select a display name for the collection, then click Create Collection.</li> </ul> <ul> <li>Return to the Endpoint tab and search for your new collection by the Display Name you gave it.</li> </ul> <ul> <li>Open the new collection in the File Manager, and you are ready to transfer files to or from your LBNL Gdrive Collection!</li> </ul>"},{"location":"data/globus-google-drive/#creating-a-shared-collection-in-gdrive","title":"Creating a Shared Collection in Gdrive","text":"<ul> <li>Find the \"LBNL Gdrive Access\" Endpoint in the Globus UI</li> <li>Click on the Collections Tab and click \"Add a Guest Collection\"</li> <li>Browse to the folder in your Google Drive that you wish to share and give it a Display Name. Click Create Collection.</li> </ul> <ul> <li>Select Add Permissions - Share With (Here the \"/\" is the root of the folder you browsed to when creating this share. In this example, it is \"My Drive/Fernsler's Demo Share/\" in user fernsler's LBNL Google Drive)</li> <li>Enter the name/email of a collaborator you wish to share with, and give them appropriate permissions. By default, read only access to shares is active. Click Add Permission to create the share.  Globus will send the user you are sharing the folder with an email and they will be able to find the Collection you shared by its Display Name in the endpoint tab search (in this example, it is \u201cShare with Wei\u201d)  </li> </ul>"},{"location":"data/globus-instructions/","title":"Globus for Lawrencium","text":""},{"location":"data/globus-instructions/#login","title":"Login","text":"<ul> <li>Open a browser and navigate to https://globus.lbl.gov .</li> <li> <p>You may choose your institution from the drop-down list. If your institution is not listed, you may Sign in using Google or ORCID id. Ideally, the email associated with your Lawrencium user account shall be used to Sing in to Globus.</p> <p></p> </li> <li> <p>If you choose LBNL as your organization, you will be directed to enter your LBNL credentials. Enter your username and password.</p> <p></p> </li> <li> <p>Enter OTP</p> <p></p> </li> </ul>"},{"location":"data/globus-instructions/#search-endpoint","title":"Search Endpoint","text":"<ul> <li> <p>Once authenticated, enter the endpoint name in the collection search bar</p> <p></p> </li> <li> <p>For example, search for <code>lbnl#lrc</code> to find lawrencium's GridFTP endpoint</p> <p></p> </li> <li> <p>Scroll down to find the <code>lbnl#lrc</code> endpoint owned by kmfensler@lbl.gov.</p> <p></p> </li> <li> <p>When the endpoint is selected, it will appear as a collection in a file manager. By default, the Globus UI will display your <code>$HOME</code> directory. Type a file path name in the navigation bar next to Path to navigate to a different directory (i.e. <code>/global/scratch/&lt;username&gt;</code>)</p> <p></p> </li> <li> <p>Click on the three dots next to the collection to view its properties.</p> </li> </ul>"},{"location":"data/globus/","title":"Globus","text":"<p>Globus is a free data transfer and storage service that lets you efficiently, securely, reliably, and quickly move large amounts of data between different resources (e.g., a personal computer, the Lawrencium cluster, Google Drive, Cloud Storage, and others) and to also share data on those resources with others.</p> <p>Globus addresses many of the common challenges faced by researchers in moving, sharing, and archiving large volumes of data. With Globus, you hand-off data movement tasks to a hosted service that manages the entire operation, monitoring performance and errors, retrying failed transfers, correcting problems automatically whenever possible, and reporting status to keep you informed of the process.</p> <p>Science IT provides many Globus endpoints to help automate data transfers. Globus endpoints are authenticated against your active LBL account credentials, however some endpoints like Lawrencium or Cloud Storage might require additional credentials or authentication methods to function properly.</p> <p>LBL's main Globus UI is available at https://globus.lbl.gov .</p> <p>Current managed endpoints with general availability are:</p> Name Globus Endpoint Name Documentation Google Drive LBNL Gdrive Access Globus for Google Drive Lawrencium lbnl#lrc Globus for Lawrencium Amazon Web Services S3 LBNL AWS S3 Collection Using the Globus AWS S3 Connector Google Cloud Storage LBNL Google Cloud Storage Collection Using the Globus Google Cloud Storage Connector <p>If you are interested in using Google Cloud or Amazon, please reach out to scienceit@lbl.gov for more information on setting up a GCP or AWS account.</p>"},{"location":"data/globus/#setting-up-a-globus-connect-personal-endpoint","title":"Setting up a Globus Connect Personal Endpoint","text":"<p>Even when there is not an LBL managed endpoint available it can still be useful to have access to the transfer and retry features of Globus. You can do this using Globus Connect Personal to configure an endpoint on your personal device. In general, it is always faster to use endpoints managed by LBL, but Globus Connect Personal can be useful for transfer to or from a local laptop or computer.</p> <p>You can find instructions for downloading and installing the Globus Connect Personal on the Globus web site .</p> <p>Globus for data transfers is highly recommended. Globus is available as a free service for any user to sign up. Please follow the instructions for access setup.</p> <p>If you see a connector you would like us to support, please send email to hpcshelp@lbl.gov.</p>"},{"location":"hpc/","title":"High Performance Computing","text":"<p>Lawrencium is the platform for the LBNL Condo Cluster Computing (LC3)  program, which provides a sustainable way to meet the midrange computing requirement for Berkeley Lab. Lawrencium is part of the LBNL Supercluster and shares the same Supercluster infrastructure. This includes the system management software, software module farm, scheduler, storage and backend network infrastructure.</p> <p>Unlike DOE computing user-facilities such as NERSC which offer leadership-tier performance but suffer from long wait times, Lawrencium provides medium-tier performance with low wait times.</p> <p>Berkeley Data Center</p> <p>Lawrencium is located at the Berkeley Data Center in Building 50B-1275. The datacenter is a 5000 sq. ft. facility dedicated for Berkeley Lab's scientific computing resources such as Lawrencium.</p>"},{"location":"hpc/#hardware-configuration","title":"Hardware Configuration","text":"<p>Lawrencium is composed of multiple generations of hardware hence it is separated into several partitions to facilitate management and to meet the requirements to host Condo projects. The following table lists the hardware configuration for each individual partition.</p> <ul> <li>Lawrencium CPU Cluster</li> <li>Einsteinium GPU Cluster</li> </ul> <p>In addition, there are several Supported Research Clusters; more information on each of these can be found by selecting the desired supported cluster under <code>Computing Systems &gt; Supported Research Clusters</code>.</p>"},{"location":"hpc/#storage-and-backup","title":"Storage and Backup","text":"<p>Lawrencium cluster users are entitled to access the following storage systems so please get familiar with them.</p> Name Location Quota Backup Allocation Description HOME <code>/global/home/users/$user</code> 30GB Yes Per User Home directory for permanant data storage GROUP-SW <code>/global/home/groups-sw/$group</code> 200GB Yes Per Group Group directory for software and data sharing with backup GROUP <code>/global/home/groups/$group</code> 400GB No Per Group Group directory for data sharing without backup SCRATCH <code>/global/scratch/users/$user</code> None No Per User Scratch directory with Lustre high performance parallel file system CLUSTERFS <code>/clusterfs/axl/$USER</code> None No Per User Private storage for AXL condo CLUSTERFS <code>/clusterfs/cumulus/$USER</code> None No Per User Private storage for CUMULUS condo CLUSTERFS <code>/clusterfs/esd/$USER</code> None No Per User Private storage for ESD condo CLUSTERFS <code>/clusterfs/geoseq/$USER</code> None No Per User Private storage for CO2SEQ condo CLUSTERFS <code>/clusterfs/nokomis/$USER</code> None No Per User Private storage for NOKOMIS condo"},{"location":"hpc/#recharge-model","title":"Recharge Model","text":"<p>LBNL has made a significant investment in developing this platform to meet the midrange computing requirement at Berkeley Lab. The primary purpose is to provide a sustainable way to host all the condo projects while meeting the computing requirements from other users. To achieve this goal, condo users are allowed to run within their condo contributions for free. However normal users who would like to use the Lawrencium cluster are subject to the LBNL recharge rate. </p> <p>Condo users who would need to run outside of their condo contributions are also subject to the same recharge rate as normal users. For this purpose, condo users will obtain either one or two projects/accounts when their accounts are created on Lawrencium, per the instruction we receive from the PI of the condo project. They would need to provide the correct project when running jobs inside or outside of their condo contributions, which will be explained in detail in the Scheduler Configuration section below. The current recharge rate is $0.01 per Service Unit (1 cent per service unit, SU). Due to the hardware architecture difference we discount effective recharge rate for older generations of hardware. Please refer to the following table for the current recharge rate for each partition.</p>"},{"location":"hpc/#cpu-partitions-recharge-rates","title":"CPU Partitions Recharge Rates","text":"Partition Shared or Exclusive SU to Core CPU Hour Ratio Effective Recharge Rate lr4 Exclusive 0 free lr5 Exclusive 0.50 $0.0050 per Core CPU Hour lr6 Exclusive 0.75 $0.0075 per Core CPU Hour lr7 Shared 1.0 $0.01 per Core CPU Hour lr8 Shared 1.0 $0.01 per Core CPU Hour lr_bigmem Exclusive 1.5 $0.015 per Core CPU Hour cf1 Exclusive 0.4 $0.004 per Core CPU Hour cm1 Shared 0.75 $0.0075 per Core CPU Hour cm2 Shared 1.0 $0.01 per Core CPU Hour es1 Shared 1.0 $0.01 per Core CPU Hour ood_inter Shared 1.0 $0.01 per Core CPU Hour"},{"location":"hpc/#gpu-partitions-recharge-rates","title":"GPU Partitions Recharge Rates","text":"Partition Shared or Exclusive SU to Core CPU Hour Ratio Effective Recharge Rate es0 Shared 0 free es1 Shared 1.0 $0.01 per Core CPU Hour <p>Usage Calculation</p> <p>The usage calculation is based on the resource that is allocated to the job instead of the actual usage of the job. For example, if a job asked for one <code>lr5</code> node with one CPU requirement (typical serial job case), and the job ran for 24 hours, since lr5 nodes are allocated exclusively to the job (please refer to the following Scheduler Configuration section for more detail), the charge that this job incurred would be: </p> <p>$0.0050/(core * hour) * 1 node * 24 cores/node * 24 hours = $2.88</p> <p>instead of: $0.0050/(core*hour) * 1 core * 24 hours = $0.12.</p>"},{"location":"hpc/#scheduler-configuration","title":"Scheduler Configuration","text":"<p>Lawrencium cluster uses SLURM to submit jobs as the scheduler to manage jobs on the cluster. To use Lawrencium through slurm, the partition (<code>lr4, lr5, lr6, es1, cm1, cm2</code> must be specified (<code>--partition=xxx</code>) along with account (<code>--account=xxx</code>). Currently the available QoS (Quality of Service)s are <code>lr_normal</code> and <code>lr_debug</code> and <code>lr_lowprio</code>. A standard fair-share policy with a decay half life value of 14 days (2 weeks) is enforced.</p> <ul> <li>For normal users to use the Lawrencium resource the proper project account, e.g., <code>--account=ac_abc</code>, is needed. The QoS <code>lr_normal</code> is also required based on the partition that the job is submitted to, e.g., <code>--qos=lr_normal</code>.</li> <li>If a debug job is desired the <code>lr_debug</code> QoS should be specified, e.g., <code>--qos=lr_debug</code> so that the scheduler can adjust job priority accordingly.</li> <li>Condo users please use the proper condo QoS, e.g., <code>--qos=condo_xyz</code>, as well as the proper recharge account <code>--account=lr_xyz</code>.</li> <li>The partition name is always required in all cases, e.g., <code>--partition=lr6</code>.</li> </ul> <p>Fair-share policy</p> <p>A standard fair-share policy with a decay half life value of 14 days (2 weeks) is enforced. All accounts are given equal shares value of 1.  All users under each account associated within a partition is subjected to decay\u2019g in priority based on the resources used and the overall parent account usage. Usage is a value between 0.0 and 1.0 that associates proportional usage of the system. A value of 0 indicates that the association is over-served. In other words that account has used its share of the resources and will be given a lower value of shares compared to users who have not used as much resources.</p> <ul> <li>Job prioritization is based on Age, Fairshare, Partition and QOS. Note: <code>lr_lowprio</code> qos jobs are not given any prioritization and some QOS have higher values than others.</li> <li>If a node feature is not provided, the job will be dispatched to nodes based on a predefined order; for <code>lr5</code> the order is: <code>lr5_c28</code>, <code>lr5_c20</code>.</li> </ul>"},{"location":"hpc/acknowledgement/","title":"Acknowledgement","text":"<p>Please acknowledge Lawrencium in your publications. A sample statement is:</p> <p>Sample Acknowledgement Statement</p> <p>This research used the Lawrencium computational cluster resource provided by the IT Division at the Lawrence Berkeley National Laboratory (Supported by the Director, Office of Science, Office of Basic Energy Sciences, of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231)</p>"},{"location":"hpc/data-transfer-node/","title":"Using the lrc-xfer DTN","text":"<p>To improve the data transfer experience of our supercluster, a separate dedicated data transfer server is available.</p> <p><code>lrc-xfer.lbl.gov</code> mounts all the cluster file systems such that users can transfer data into/from any cluster filesystem. Also NERSC HPSS data transfer utilities like <code>hsi</code> and <code>htar</code> are configured to work on this server.</p>"},{"location":"hpc/data-transfer-node/#data-transfer-examples-on-linux","title":"Data Transfer Examples On Linux","text":"<p>Transfer data from a local machine to Lawrencium</p> File transferFolder transfer <pre><code>scp file-xxx $USER@lrc-xfer.lbl.gov:/global/home/users/$USER\n</code></pre> <pre><code>scp -r dir-xxx $USER@lrc-xfer.lbl.gov:/global/scratch/$USER\n</code></pre> <p>Transfer from Lawrencium to a local machine</p> <pre><code>scp $USER@lrc-xfer.lbl.gov:/global/scratch/$USER/file-xxx ~/Desktop\n</code></pre> <p>Transfer from Lawrencium to Another Institute</p> <pre><code>ssh $USER@lrc-xfer.lbl.gov   # DTN\n</code></pre> <pre><code>scp -r $USER@lrc-xfer.lbl.gov:/file-on-lawrencium $USER@other-institute:/destination/path/\n</code></pre>"},{"location":"hpc/data-transfer-node/#rsync-data-transfer-and-backup-tool","title":"Rsync: data transfer and backup tool","text":"<pre><code>rsync -avpz file-at-local $USER@lrc-xfer.lbl.gov:/global/home/user/$USER \n</code></pre>"},{"location":"hpc/data-transfer-node/#data-transfer-examples-on-windows","title":"Data Transfer Examples On Windows","text":"<ul> <li>WinSCP: SFTP client and FTP client for Microsoft Windows</li> <li>FileZilla: multi-platform program via SFTP</li> </ul>"},{"location":"hpc/faqs/","title":"FAQs","text":"How do I get a user account on the Lawrencium cluster? <p>Principal Investigators (PIs) can sponsor researchers, students and external collaborators for cluster accounts. Account requests and approval are done through the MyLRC portal. Either the PI or a user can place a user account creation request on the MyLRC portal. Please see the MyLRC documentation  to learn how to submit a request. Upon request, an automatic email will be sent to your PI for approval. When the PI approves the request, it will be processed and the user is notified through email upon account availability.</p> How do I submit my first job? <p>Login to the cluster using any of the terminal options of your choice. You may login to cluster using the server name <code>lrc-login.lbl.gov</code>. Use your user name and PIN+OTP combination to login successfully. Upon login you will be end up on one of the login nodes in your home directory. Please do not submit jobs on the login nodes. You would request a compute node either using an interactive or batch slurm session. You need to know your slurm association before scheduling a slurm session. Check out slurm job submission examples here. Depending on type of job for example CPU only, GPU, MPI, serial, you could visit the slurm script examples on this page.</p> How do I transfer data to and from the cluster? <p>For more details, please see the examples on the Using the lrc-xfer DTN page.</p> What is the maximum runtime / walltime you can assign a job? <p>It depends on the <code>qos</code> and the information can be obtained using the following command</p> <pre><code>sacctmgr show qos name=lr_normal,lr_debug,lr_interactive,cm1_debug,cm1_normal,es_debug,es_normal,cf_debug,cf_normal,es_lowprio,cf_lowprio format=name,maxtres,maxwall,mintres\n</code></pre> <p>The maximum runtime / walltime is shown on the <code>MaxWall</code> column. The output may look like the following:</p> <pre><code>      Name       MaxTRES     MaxWall       MinTRES \n---------- ------------- ----------- ------------- \n  lr_debug        node=4    03:00:00         cpu=1 \n lr_normal                3-00:00:00         cpu=1 \n cf_normal       node=64  3-00:00:00         cpu=1 \n  cf_debug        node=4    01:00:00         cpu=1 \n es_normal       node=64  3-00:00:00 cpu=2,gres/g+ \n  es_debug        node=4    03:00:00 cpu=2,gres/g+ \n cm1_debug        node=4    01:00:00         cpu=1 \ncm1_normal       node=64  3-00:00:00         cpu=1 \nes_lowprio                           cpu=2,gres/g+ \ncf_lowprio                                         \nlr_intera+        cpu=32  3-00:00:00               \n</code></pre>"},{"location":"hpc/getting-started/","title":"Getting Started","text":""},{"location":"hpc/getting-started/#project-accounts","title":"Project Accounts","text":"<p>There are three primary ways/projects to obtain access to Lawrencium:</p> <ul> <li>PI Computing Allowance (PCA): free 500K SUs annual renewable</li> <li>Condo: purchase and contribute Condo nodes to the Lawrencium cluster</li> <li>Recharge: charged at a minimal recharge rate roughly at $0.01/SU</li> </ul> <p>How to get a Project Account on Lawrencium?</p> <p>Project Group Directories</p> <p>Project group directories are not created by default. If you would like to create group directories where your group members can share data and software, please send a request to hpcshelp@lbl.gov.</p>"},{"location":"hpc/getting-started/#user-accounts","title":"User Accounts","text":"<p>You must have a user account to gain access to the Lawrencium cluster. </p> <p>How to request a User Account and Submit User Agreement?</p>"},{"location":"hpc/getting-started/#logging-in","title":"Logging in","text":"<p>You'll need to generate and enter a one-time password each time you log in.</p> <p>Click here for more information on logging in to Lawrencium</p>"},{"location":"hpc/getting-started/#data-movement-and-storage","title":"Data Movement and Storage","text":"<p>To transfer data from other computers into - or out of - your various storage directories, you can use protocols and tools like SCP, STFP, FTPS, and Rsync. If you\u2019re transferring lots of data, the web-based Globus Connect tool is typically your best choice: it can perform fast, reliable, unattended transfers. </p> <p>The LRC supercluster\u2019s dedicated Data Transfer Node is <code>lrc-xfer.lbl.gov</code>. For more information on getting your data onto and off of Lawrencium, please see Data Transfer.</p>"},{"location":"hpc/getting-started/#software-module-farm-and-environment-modules","title":"Software Module Farm and Environment Modules","text":"<p>A lot of software packages and tools are already built as Software Module Farm and provided for your use. These software packages and tools can be loaded and unloaded via Environment Module commands. For details see Software Module Farm and Module Management.</p>"},{"location":"hpc/getting-started/#running-jobs","title":"Running Jobs","text":"<p>When you log into a cluster, you\u2019ll land on one of several login nodes. Here you can edit scripts, compile programs etc. However, you should not be running any applications or tasks on the login nodes, which are shared with other cluster users. Instead, use the SLURM job scheduler to submit jobs that will be run on one or more of the cluster\u2019s many compute nodes. For details see Slurm Overview and Example Scripts.</p>"},{"location":"hpc/getting-started/#open-ondemand","title":"Open OnDemand","text":"<p>We provide interactive Apps, such as Jupyter notebooks, RStudio, MatLab, through the browser-based Open OnDemand service at https://lrc-ondemand.lbl.gov . Use your LRC username and PIN+one-time password(OTP).</p>"},{"location":"hpc/status/","title":"HPC Clusters @ Berkeley Lab - Live Status","text":""},{"location":"hpc/accounts/loggingin/","title":"Logging in","text":"<p>Multi-Factor Authentication (MFA)</p> <p>Please make sure you have configured Multi-Factor Authentication (MFA) before logging in for the first time.</p> <p>You\u2019ll need to generate and enter a one-time password each time that you log in. You\u2019ll use an application called Google Authenticator to generate these passwords, which you can install and run on your smartphone and/or tablet. For instructions on setting up and using Google Authenticator, see Multi-Factor Authentication. Once you have your PIN+OTP set up you can login to cluster using a ssh client of your choice or Linux/Mac terminal as </p> <pre><code>ssh username@lrc-login.lbl.gov\n</code></pre> <p>You will be prompted to enter your password. Enter your PIN+OTP without any spaces. For example if your pin is <code>0123</code> and OTP is <code>456789</code>, then you will type it as <code>0123456789</code>. Note that the characters won\u2019t appear on the screen.</p> <p>Running jobs</p> <p>The login nodes should not be used for running jobs. They should only be used to write scripts and submit jobs to the compute nodes.</p> <p>More details on writing job scripts and submitting them can be found here.</p>"},{"location":"hpc/accounts/mfa/","title":"Multi-Factor Authentication (MFA)","text":"<p>Link to Token Management Page</p> <p>Visit the Token Management web page  to manage your MFA; detailed instructions are given below.</p>"},{"location":"hpc/accounts/mfa/#introduction","title":"Introduction","text":"<p>All users are required to use Multi-Factor Authentication (MFA) for logging into IT HPC resources such as the Lawrencium cluster and other scientific computing clusters managed by HPCS. MFA provides greater protection than regular passwords against phishing and other modern threats to your digital security. With MFA, you authenticate using your password plus a \"one-time password\" (OTP). As the name implies, you can use an OTP only once.</p> <p></p> <p>All users are required to install and use an Authenticator app in their smart phones and configure it to generate OTPs.  There are many such apps, some of the popular ones and known to work are Google Authenticator (GA), Microsoft Authenticator, and Authy.  (Note, Duo is supposed to work, but at least two users have run into time sync problem between the Duo implementation and the LBL Radius server, thus at this time, Duo is NOT recommended).</p> <p></p> <p>There are also desktop apps, but they somewhat negate the advantage of MFA being \"something you have with you\". But if you need a dekstop app, you can try to use the Authy desktop app by using the instructions in this link.</p> <p>YubiKey</p> <p>Lastly, it is also possible to use a YubiKey  as your MFA.  This has a significant setup cost and in person verification; if you are interested in this,  please email HPCS support at hpcshelp@lbl.gov for additional assistance.</p>"},{"location":"hpc/accounts/mfa/#mfa-instructions","title":"MFA Instructions","text":""},{"location":"hpc/accounts/mfa/#step-1-download-and-install-google-authenticator-on-a-mobile-device","title":"Step 1: Download and install Google Authenticator on a mobile device.","text":"<ul> <li>In the Google Play store or iOS App Store on your smartphone or tablet, search for and install Google Authenticator (GA), Microsoft Authenticator, Authy and Duo.</li> </ul>"},{"location":"hpc/accounts/mfa/#step-2-visit-and-login-to-otp-token-management-interface","title":"Step 2: Visit and Login to OTP Token Management Interface","text":"<ul> <li> <p>Berkeley Lab users can access the interface by clicking \u2018Berkeley Lab Login\u2019 in the top section.</p> </li> <li> <p>External Users (Non Berkeley Lab users) should have linked one of their personal accounts (Facebook or Google or UC Berkeley) with the LRC HPC Cluster account and using that linked personal account credentials can access the token management interface.</p> </li> <li> <p>External users must link their account to the email address you used while requesting an account. You would request an email to link an account. If you haven\u2019t received an email or the link is expired, then you can request a linking email using the MyLRC portal. To do so, please go to your profile on the upper right-hand side. On your profile page at the bottom, you will see a button Request Linking Email. Please click on it, and you will get an email within half an hour. If you don\u2019t get the email, please get in touch with us at hpcshelp.lbl.gov.</p> </li> </ul>"},{"location":"hpc/accounts/mfa/#step-3-after-login-create-a-hpc-clusterlinux-workstation-token","title":"Step 3: After login create a HPC Cluster/Linux Workstation Token","text":"<ul> <li>All HPCS managed Clusters and Linux workstations use the HPC Cluster/Linux Workstation token shown in the second section of the \u2018Token Management\u2019 page (toward the bottom).</li> </ul> <ul> <li>Click on the \u2018Add Token\u2019 link and follow instructions. </li> </ul> <p>Important</p> <p>Remember the PIN that you are setting which you will use every time you access the resource. PINs can be changed or updated later from this interface itself.</p> <ul> <li>After you've successfully created your new token, a QR code for that token will then be displayed.</li> </ul> <p></p>"},{"location":"hpc/accounts/mfa/#step-4-scan-the-2-d-qr-code","title":"Step 4 Scan the 2-D QR code","text":"<ul> <li>Back on your smartphone or tablet, from the menu of the Google Authenticator app, select \u2018Add an account\u201d and then \u201cScan a barcode\u201d. This will store the token in GA app and its now ready to generate One Time Passwords.</li> </ul> <p>Note</p> <p>If your device does not already have a QR code reader app installed, the Google Authenticator app may first lead you through the process of installing one.</p> <p>Important</p> <p>When you access the resource remember to type the token PIN first followed the OTP from the GA app at the password prompt.</p> <p>For instance, if your PIN was 9999 (hint: don\u2019t use this example as your own PIN!), and the one time-password currently displayed by Google Authenticator was 123456, you\u2019d enter the following at the Password prompt: Password: 9999123456</p>"},{"location":"hpc/accounts/mfa/#troubleshooting","title":"Troubleshooting","text":"<p>If you\u2019ve already set up your token but are unable to log into the cluster successfully \u2013 here\u2019s what to try:</p> <p>Tip 1</p> <p>Make sure you\u2019re including the PIN as part of your password</p> <p>At the Password: prompt, make sure that you\u2019re entering your token PIN, followed immediately by the 6-digit one-time password from Google Authenticator.</p> <p>Tip 2</p> <p>Wait to enter the one-time password until a new one has just been displayed</p> <p>If the \u2018countdown clock\u2019 indicator in the Google Authenticator app is nearing its end, signifying that the existing password is about to expire, try waiting until a new one-time password has been displayed. Then enter that new password, immediately after your PIN, at the Password: prompt.</p> <p>Tip 3</p> <p>Check that, in your SSH command or in the configuration for your SSH application, you\u2019re using your correct login name (i.e., your Linux user name) on the cluster</p> <p>Tip 4</p> <p>Check that, in your SSH command or in the configuration for your SSH application, you\u2019re using the correct hostname for the cluster\u2019s front-end/login nodes, lrc-login.lbl.gov, or for its Data Transfer Node, lrc-xfer.lbl.gov</p> <p>Tip 5</p> <p>Test \u2013 and if needed, reset \u2013 your token or its PIN</p> <p>Visit the Token Management web page  to log in to this Token Management page.</p> <p>A list of one or more tokens should then be displayed. From this list, find your relevant token: the one that you entered into Google Authenticator on the smartphone or tablet you\u2019re currently using. (If you want to check this further, the \u201cTOTP number\u201d that appears in the box for your token, on the Token Management web page, should match the TOTP number in Google Authenticator\u2019s window on your device.)</p> <p>If there\u2019s only a \u201cReset\u201d option in that token\u2019s box, click that link. Then proceed to the next step, below.</p> <p>If there\u2019s a \u201cTest\u201d option, click that link, then enter your PIN followed immediately by your Google Authenticator 6-digit one-time password, and click the \u201cTest Now\u201d button.</p> <p>If your test(s) fail, click \u201cDone\u201d. Then click the \u201cReset PIN\u201d link and reset your PIN. (You can even \u2018reset\u2019 it to your current PIN.)</p> <p>Try the \u201cTest\u201d option once again.</p> <p>Once you get a successful test of your PIN plus one-time password on this web page, you can try logging into cluster once again and see if you\u2019re successful there, as well.</p> <p>Tip 6</p> <p>Try creating a brand new token and add the new token to Google Authenticator, as described in the instructions above. (Before or after doing this, you can delete your existing token \u2013 both on the LBL Token Management web page and in the Google Authenticator app on your device \u2013 to avoid any confusion with the new token.)</p> <p>Tip 7</p> <p>If none of the above tips give you a clue on what is not working, try to SSH to LRC resources from a different IP address i.e from a different computer or laptop. If that works email the IP address from where its not working to LRC support@hpcshelp@lbl.gov .</p>"},{"location":"hpc/accounts/project-accounts/","title":"Project Accounts","text":"<p>The Lawrencium cluster is open to all Berkeley Lab researchers needing access to high performance computing. Research collaborations are also welcome provided that there is a LBNL PI.</p> <p>LBNL PIs wanting to obtain access to Lawrencium for their research project will need to complete the project request at myLRC portal , giving the details of the research activity along with a list of anticipated users. A unique group name will be created for the project and associated users. This group name will be used to setup allocations and report usage.</p> <p>There are three primary ways to obtain access to Lawrencium:</p> <ol> <li> <p>LBNL PIs: requesting a block of no-cost computing time via a PI Computing Allowance (PCA). This option is currently offered to all eligible Berkeley Lab PIs. For additional details on please see PI Computing Allowance .</p> </li> <li> <p>Condo projects: purchasing and contributing Condo nodes to the cluster. This option is open to any Berkeley Lab staff, and provides ongoing, priority access to you and your research affiliates who are members of the Condo. For details, please see Condo Cluster Service .</p> </li> <li> <p>Recharge use: Berkeley lab researchers who want to use Lawrencium cluster at a minimal recharge rate, roughly at $0.01/SU. For details, please see Recharge Allocation Computing Allowance.</p> </li> </ol> <p>To request a PCA, Condo or Recharge project on Lawrencium, please send your requests at myLRC portal . Make sure choosing the desired project type on the form.</p>"},{"location":"hpc/accounts/project-accounts/#changing-your-projectid","title":"Changing Your ProjectID","text":"<p>If your projectID associated with project accounts on Lawrencium expires or becomes invalid, you can request of changing your projectID by sending us email at hpcshelp@lbl.gov.</p>"},{"location":"hpc/accounts/project-accounts/#allocations","title":"Allocations","text":"<p>Computer Time: We are currently not using an allocation process to allocate compute time to individual projects. Instead, usage and priority will be regulated by a scheduler policy intended to provide a level of fairness across users. If needed, a committee consisting of scientific division representatives will review the need for allocations if demand exceeds supply.</p> <p>Cost: There is a nominal charge of $25/mo/user for the use of Lawrencium to cover the costs of home directory storage and backups. PCA Project accounts are not charged for usage. Recharge accounts are charged $0.01/SU for compute. Account fees and cpu usage will appear as LRCACT and LRCCPU in the LBL Cost Browser.</p> <p>Storage: Home directory space will have a quota set at 20GB per user. Users may also use the <code>/clusterfs/lawrencium</code> shared filesystem which does not have a quota; this file system is intended for short term use and should be considered volatile. Backups are not performed on this file system. Data is subject to periodic purge policy wherein any files which are not accessed with in the last 14 days will be deleted. Users should make sure to have a back up of these files to some external permanent storage as soon as they are generated on the cluster.</p> <p>Lustre: Lustre parallel file system is also now available for Lawrencium cluster users. The file system is built with 4 OSS and 15 OST servers with a capacity of 1.8PB. The default striping is set to 4 OSTs with strip size of 1 MB. All the Lawrencium cluster users will receive a directory created under <code>/clusterfs/lawrencium</code> with the above default stripe values set. This is a scratch file system, so its mainly intended for storing large input or output files for running jobs and for all the parallel I/O needs on the Lawrencium cluster. This file system is intended for short term use and should be considered volatile. Backups are not performed on this file system. Data in scratch is subject to periodic purge policy wherein any files which are not accessed with in the last 14 days will be deleted. Users should make sure to have a back up of these files to some external permanent storage as soon as they are generated on the cluster.</p>"},{"location":"hpc/accounts/project-accounts/#acknowledgements","title":"Acknowledgements","text":"<p>Please acknowledge Lawrencium in your publications. A sample statement is:</p> <p>Sample Acknowledgement Statement</p> <p>This research used the Lawrencium computational cluster resource provided by the IT Division at the Lawrence Berkeley National Laboratory (Supported by the Director, Office of Science, Office of Basic Energy Sciences, of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231)</p>"},{"location":"hpc/accounts/user-accounts/","title":"User Accounts","text":""},{"location":"hpc/accounts/user-accounts/#how-to-get-a-user-account-on-lawrencium-how-to-add-additional-users-to-your-project","title":"How to get a user account on Lawrencium / How to add additional users to your project?","text":"<p>Accounts are added on a first come, first served basis upon approval of the PI for the project. For security reasons, access to Lawrencium will be through the use of one-time password tokens. Users will be required to complete  user account requests at myLRC portal  in order to get their one-time password token generator and their account.</p> <p>Closing User Accounts</p> <p>The PI for the project or the main contact is responsible for notifying HPCS to close user accounts and the disposition of the user\u2019s software, files and data. In some cases, users share software and data from their home directory and others may depend on them. For this reasons, only account terminations have to be requested by PI, the main account or the user of the account. Users accounts are not automatically deactivated upon termination of an employee because many people change their employment status, but remain engaged with the project. Send your requests at myLRC portal .</p> <p>Questions regarding requesting new or removing accounts can be directed to hpcshelp@lbl.gov.</p>"},{"location":"hpc/openondemand/jupyter-ai-ollama/","title":"Using Jupyter AI with Ollama","text":"<p>Please see Ollama with Jupyter and VS Code</p>"},{"location":"hpc/openondemand/jupyter-server/","title":"Jupyter Server","text":"<p>The Jupyter Notebook is a web application that enables you to create and share documents that can contain a mix of live code, equations, visualizations, and explanatory text. This is an introduction to using Jupyter notebooks on Lawrencium.</p> <p>Before getting started, make sure you have access to the Lawrencium cluster.</p> <p>As described next, you can start a Jupyter notebook via the Open OnDemand service, which allows you to operate completely via your web browser on your local computer (e.g., your laptop).</p>"},{"location":"hpc/openondemand/jupyter-server/#jupyter-notebooks-on-open-ondemand","title":"Jupyter notebooks on Open OnDemand","text":""},{"location":"hpc/openondemand/jupyter-server/#running-a-notebook","title":"Running a notebook","text":"<ol> <li> <p>Connect to https://lrc-ondemand.lbl.gov .</p> </li> <li> <p>After logging in, you will get to the Open OnDemand welcome screen. Click the Interactive Apps pulldown.</p> </li> <li> <p>Choose the Jupyter Server option from the list of apps. Choose the Jupyter Server - interactive for exploration/debugging only if you are writing/debugging code and not doing any computationally intensive tasks. </p> </li> <li> <p>Fill out the form presented to you and click on Launch. An example of filling this form is shown in the next section.</p> </li> <li> <p>Once the server is ready, you will be able to click on the Connect to Jupyter button to get a jupyter notebook.</p> </li> </ol>"},{"location":"hpc/openondemand/jupyter-server/#example-launch-a-jupyter-server-on-the-es1-gpu-parition","title":"Example: Launch a Jupyter Server on the ES1 GPU parition","text":"<ol> <li> <p>Select the following parameters to lauch a jupyter server on one GPU card of a A40 GPU node using a normal priority queue (with 16 CPU cores):</p> <ul> <li>SLURM Partition: es1</li> <li>Name of SLURM Quality of Service (QoS): es_normal</li> <li>Number of nodes: 1</li> <li>Select GPU Type from dropdown: es1: NVidia A40 (40 GB) 1-4x</li> <li>Select number of GPU cards to use: 1</li> <li>Number of CPU cores per Node: 16 </li> </ul> <p>Please also choose or enter the SLURM Project/Account Name, the Wall Clock Time, and Name of the job according to your needs.</p> <p>Example form for choosing a A40 GPU card</p> <p></p> </li> <li> <p>Upon clicking Launch, you may have to wait for the requested resource to be allocated. </p> </li> <li> <p>When the server is ready, click on the Connect to Jupyter button to open your jupyter server session.      </p> </li> <li> <p>After clicking on Connect to Jupyter, you will enter the classic Jupyter or Jupyterlab environment. </p> <ul> <li>Under File &gt; New &gt; Notebook, you will find several Jupyter kernels with different Python versions and packages that you can choose according to your requirements. These include:<ul> <li>Python 3 (ipykernel) - python through <code>anaconda3/2024.02-1-11.4</code> module</li> <li>torch 2.3.1 py3.11.7 - PyTorch 2.3.1 through <code>ml/pytorch/2.3.1-py3.11.7</code> module</li> <li>tf 2.15.0 py3.10.0 - TensorFlow 2.15.0 through <code>ml/tensorflow/2.15.0-py3.10.0</code> module</li> </ul> </li> </ul> </li> <li> <p>You can have your session continue to operate in the background by selecting the Logout button (upper right hand corner) on Open OnDemand.</p> </li> <li> <p>To terminate a running Notebook, select the My Interactive Sessions tab on the Open OnDemand menu and click on Delete.</p> </li> </ol> <p>Further information about working with Jupyter Notebooks can be found in the Jupyter Documentation  and JupyerLab Documentation </p>"},{"location":"hpc/openondemand/ollama-jupyter-vscode/","title":"Using Ollama with Jupyter and VS Code","text":"<p>The Ollama - JupyterAI &amp; VS Code Continue app can be used on LRC Open Ondemand for running LLMs locally on Lawrencium compute resources. This can be useful for prototying applications that make use of LLMs, or for general experimentation.</p> <p>To use the app, take the following steps:</p> <ol> <li> <p>After clicking on the app on the Interactive Apps menu of LRC Open Ondemand, fill out the form with your requirements. Below is an example that will request one V100 GPU for 3 hours. GPU nodes (<code>es0</code> or <code>es1</code> partition) are recommended for running LLM models. Choose the partition, GPU type and number of GPUs according to your needs. </p> Example form for choosing one V100 GPU card on es1 partition <p></p> </li> <li> <p>Click Launch. Upon clicking Launch, you may have to wait for the requested resource to be allocated.</p> </li> <li>When the server is ready, you will get two buttons: Connect to Jupyter and Connect to VS Code as shown in the image below.     </li> </ol>"},{"location":"hpc/openondemand/ollama-jupyter-vscode/#ollama-on-jupyter","title":"Ollama on Jupyter","text":"<p>If you click on Connect to Jupyter, you will get a Jupyter Lab instance with Jupyter AI  extension. To chat using the default Ollama model, click on the Jupyter AI chat interface on the left-side of the JupyterLab workspace. To change models or settings, click on the settings icon of the Jupyter AI interface on the top right corner.</p> Jupyter AI Interface <p></p>"},{"location":"hpc/openondemand/ollama-jupyter-vscode/#changing-model-on-jupyter-ai","title":"Changing model on Jupyter AI","text":"<p>To change the model, you will need to type in the model name from the list of currently available models; for example: <code>devstral:24b</code>, <code>gemma3:12b</code>. A complete list can be obtained by using the <code>ollama list</code> command on a terminal (File &gt; New &gt; Terminal).</p> <code>ollama list</code> <pre><code>[user@hostname ~]$ ollama list\nNAME                     ID              SIZE      MODIFIED    \ndevstral:24b             c4b2fa0c33d7    14 GB     6 days ago     \ncodegemma:2b             926331004170    1.6 GB    11 days ago    \nnomic-embed-text:v1.5    0a109f422b47    274 MB    4 weeks ago    \ndeepseek-coder:6.7b      ce298d984115    3.8 GB    4 weeks ago    \ndeepseek-coder:1.3b      3ddd2d3fc8d2    776 MB    4 weeks ago    \nllama3.2:1b              baf6a787fdff    1.3 GB    4 weeks ago    \nqwen3:1.7b               458ce03a2187    1.4 GB    4 weeks ago    \nqwen3:30b-a3b            2ee832bc15b5    18 GB     4 weeks ago    \nqwen3:8b                 e4b5fd7f8af0    5.2 GB    4 weeks ago    \ndeepseek-r1:8b           28f8fd6cdc67    4.9 GB    4 weeks ago    \ndeepseek-r1:7b           0a8c26691023    4.7 GB    4 weeks ago    \ndeepseek-r1:1.5b         a42b25d8c10a    1.1 GB    4 weeks ago    \ngemma3:4b                a2af6cc3eb7f    3.3 GB    4 weeks ago    \ngemma3:12b               f4031aab637d    8.1 GB    4 weeks ago    \ngemma3:12b-it-qat        5d4fa005e7bb    8.9 GB    4 weeks ago    \ngemma3:1b                8648f39daa8f    815 MB    4 weeks ago    \n</code></pre>"},{"location":"hpc/openondemand/ollama-jupyter-vscode/#using-ollama-python-library-on-jupyter-notebooks","title":"Using <code>ollama</code> python library on Jupyter notebooks","text":"<p>You can use <code>ollama</code> python  module to interact with Ollama in a notebook using the default <code>Python 3 (ipykernel)</code> kernel. For example:</p> <p><code>ollama-python</code> example</p> <pre><code>import ollama\nimport os\nclient = Client(host=os.environ[\"OLLAMA_HOST\"])\nresponse = client.chat(model='llama3.2:1b', \n                    messages=[{'role': 'user', 'content': 'Hello'}])\n</code></pre>"},{"location":"hpc/openondemand/ollama-jupyter-vscode/#ollama-on-vs-code","title":"Ollama on VS Code","text":"<p>If you click on Connect to VS Code, you will get a VS Code server instance with Continue  extension. You can use the Continue Chat feature by clicking on the Continue button on the left-side of VS Code workspace.</p> VS Code Continue Interface <p></p>"},{"location":"hpc/openondemand/overview/","title":"Open OnDemand Overview","text":"<p>We provide various interactive Apps through the browser-based Open OnDemand service available at https://lrc-ondemand.lbl.gov .</p> <p>The available Apps/services include:</p> <ul> <li>Jupyter notebooks</li> <li>Ollama</li> <li>RStudio</li> <li>Matlab</li> <li>VS Code</li> <li>File browsing</li> <li>Slurm job listing</li> <li>Terminal/shell access (under the \"Clusters\" tab)</li> </ul>"},{"location":"hpc/openondemand/overview/#logging-in","title":"Logging In","text":"<ol> <li>Visit https://lrc-ondemand.lbl.gov  in your web browser.</li> <li>Login using CILogon. At the login page, please select the appropriate institute. If you have a Berkeley Lab identity please select Lawrence Berkeley National Laboratory and use your Berkeley Lab Identity to login to Open OnDemand.</li> </ol>"},{"location":"hpc/openondemand/overview/#service-unit-charges","title":"Service Unit Charges","text":"<p>Open OnDemand apps may launch Slurm jobs on your behalf when you request sessions on a slurm partition. Open OnDemand refers to these jobs as \"interactive sessions.\" Since these are just Slurm jobs, service units are charged for interactive sessions the same way normal jobs are charged.</p> <p>Interactive, for exploration/debugging mode</p> <p>Sessions can be run on <code>.ood0</code> nodes by choosing <code>interactive, for exploration/debugging</code> versions of the apps. Nodes ending in <code>.ood0</code> are shared nodes that are provided for low-intensity jobs. These should be treated like login nodes (that is, intensive computation is not allowed). Interactive sessions running on <code>.ood0</code> nodes are charged at 1 SU per CPU-hour.</p> <p>Job time is counted for interactive sessions as the total time the job runs. The job starts running as soon as a node is allocated for the job. The interactive session may still be running even if you do not have it open in your web browser. You can view all currently running interactive sessions under My Interactive Sessions. When you are done, you may stop an interactive session by clicking \u201cDelete\u201d on the session.</p> <p>There are several ways to monitor usage:</p> <ul> <li>Since Open OnDemand submits jobs through Slurm, you can monitor usage as you would monitor your regular Slurm Jobs.</li> <li>View currently running (and recent) sessions launched by Open OnDemand under <code>My Interactive Sessions</code>.</li> <li>View all currently running jobs under <code>Jobs &gt; Active Jobs</code>.</li> </ul>"},{"location":"hpc/openondemand/overview/#using-open-ondemand","title":"Using Open OnDemand","text":"<p>Here are some of the services provided via Open OnDemand.</p> <p>Services on Open OnDemand</p> Files AppView Active JobsShell Access <p>Access the Files App from the top menu bar under Files &gt; Home Directory. Using the Files App, you can use your web browser to:</p> <ul> <li>View files in the Lawrencium filesystem.</li> <li>Create and delete files and directories.</li> <li>Upload and download files from the Lawrencium filesystem to your computer.</li> <li>We recommend using Globus for large file transfers.</li> </ul> <p></p> <p></p> <p>View and cancel active Slurm jobs from Jobs &gt; Active Jobs. This includes jobs started via <code>sbatch</code> and <code>srun</code> as well as jobs started via Open OnDemand.</p> <p></p> <p></p> <p>Open OnDemand allows Lawrencium shell access from the top menu bar under Clusters &gt; LRC Shell Access.</p> <p></p> <p></p>"},{"location":"hpc/openondemand/overview/#interactive-apps","title":"Interactive Apps","text":"<p>Additionally, Open OnDemand provides the following interactive apps.</p> <ul> <li>Desktop App</li> <li>Jupyter Server</li> <li>MATLAB</li> <li>RStudio Server</li> <li>VS Code Server</li> </ul> <p>Click on a tab below to learn more about these interactive apps.</p> <p>Interactive Apps on Open OnDemand</p> Desktop AppJupyter ServerMATLABRStudio ServerVS Code Server <p>The Desktop App allows you to launch an interactive desktop on the Lawrencium cluster. You will be able to launch GUI applications directly on the desktop.</p> <p>Steps:</p> <ul> <li>Select Desktop from the Interactive Apps menu.</li> <li>Provide the job specifications you want for the Desktop app.</li> <li>Once Desktop is ready, click Launch Desktop and the Desktop will open in a new tab.</li> </ul> <p>See the Jupyter documentation page for instructions on using Jupyter notebooks via Open OnDemand. This service replaces the JupyterHub service that we formerly provided.</p> <p>Steps:</p> <ul> <li>Select Jupyter Server from the Interactive Apps menu.</li> <li>Provide the job specifications you want for the Jupyter server.</li> <li>Once Jupyter is ready, click Connect to Jupyter to access your Jupyter session.</li> </ul> <p>The MATLAB app allows your to use MATLAB GUI on Lawrencium cluster.</p> <p>Steps:</p> <ul> <li>Select MATLAB from the Interactive Apps menu.</li> <li>Specify the amount of time you would like the MATLAB sessions to run.</li> <li>Once the MATLAB session is ready, click Launch MATLAB to access MATLAB GUI.</li> </ul> <p>The RStudio server allows you to use RStudio on Lawrencium cluster.</p> <p>Steps:</p> <ul> <li>Select RStudio Server from the Interactive Apps menu.</li> <li>Provide the job specification you want for the RStudio server.</li> <li>Once RStudio is ready, click Connect to RStudio to access RStudio.</li> </ul> <p>The VS Code server allows you to use VS Code on Lawrencium cluster.</p> <p>Steps:</p> <ul> <li>Select VS Code Server from the Interactive Apps menu.</li> <li>Provide the job specification you want for the VS Code server.</li> <li>Once VS Code Server is ready, click Connect to VS Code to access VS Code.</li> </ul> <p>Job run time</p> <p>Service units are charged based on job run time. The job may still be running if you close the window or log out. When you are done, shut down an interactive app by clicking \"Delete\" on the session under My Interactive Sessions.</p>"},{"location":"hpc/openondemand/overview/#troubleshooting-open-ondemand","title":"Troubleshooting Open OnDemand","text":""},{"location":"hpc/openondemand/overview/#common-problems","title":"Common problems","text":"Problem: Open OnDemand login pop-up box keeps reappearing <p>If you have trouble logging into OOD (including if the login pop-up box keeps reappearing after you enter your username and password), you may need to make sure you have completely exited out of other OOD sessions. This could include closing browser tab(s)/window(s), clearing your browser cache and clearing relevant cookies. You might also try running OOD in an incognito window (or if using Google Chrome, in a new user profile).</p>"},{"location":"hpc/openondemand/overview/#general-information-for-troubleshooting","title":"General information for troubleshooting","text":"<p>Logs and scripts for each interactive session with Open OnDemand are stored in:</p> <pre><code>~/ondemand/data/sys/dashboard/batch_connect/sys\n</code></pre> <p>There are directories for each interactive app type within this directory. For example, to see the scripts and logs for a Jupyter session, you might look at the files under:</p> <pre><code>~/ondemand/data/sys/dashboard/batch_connect/sys/lrc_jupyter/output/da19101d-70b0-43c1-84ff-7d9f0e739419\n</code></pre>"},{"location":"hpc/openondemand/packages-kernels/","title":"Adding Packages and Kernels","text":""},{"location":"hpc/openondemand/packages-kernels/#installing-python-packages","title":"Installing Python Packages","text":"<p>A variety of standard Python packages (such as numpy, scipy, matplotlib and pandas) are available automatically on <code>anaconda3</code> module. To see what packages are available, open a Terminal in the Jupyter server or open a Terminal on Lawrencium in the usual fashion. Then load the <code>anaconda3</code> module and list the installed packages: <pre><code>module load anaconda3\nconda list\n</code></pre></p> <p>pip vs python -m pip</p> <p>To reduce potential environment mismatch (especially in the presence of multiple python installations), it is recommended to use <code>python -m pip</code> rather than <code>pip</code>.</p> <p>You can use <code>pip</code> to install or upgrade packages and then use them in a Jupyter notebook, but you will need to make sure to install the new versions or additional packages in your <code>home</code> or <code>scratch</code> directories because you do not have write permissions to the module directories. You can use </p> <p><pre><code>python -m pip install --user $PACKAGENAME \n</code></pre> to install the python package to <code>$HOME/.local</code>.</p> <p>So, if you need to install additional packages, you can load the desired Python module and then use <code>pip</code> to install in your <code>home</code> directory. For example, you can install the <code>cupy</code> package with:</p> <pre><code>module load anaconda3/2024.02\nmodule load gcc/11.4.0\nmodule load cuda/12.2.1\npython -m pip install --user --no-cache-dir cupy-cuda12x\n</code></pre> <p>Package installed in this manner in your <code>$HOME/.local/lib/python3.xx</code> will be available to the Python 3 (ipykernel) jupyter kernel provided through the <code>anaconda3/2024.02</code> module.</p> <p>You can also install packages in a virtual environment or a conda environment and create a kernel associated with that environment. See examples in the next sections.</p>"},{"location":"hpc/openondemand/packages-kernels/#adding-new-kernels","title":"Adding New Kernels","text":"<p>Jupyter supports notebooks in dozens of languages, including Python, R, and Julia. Not all of these languages or packages are supported by default in our Open OnDemand jupyter server. The ability to create custom kernels is useful if you need to create your own kernel for a language that is not supported by default or if you want to customize the environment, for example create a jupyter kernel for a virtual environment or a conda environment. </p> <p>To list the available jupyter kernels:</p> <pre><code>jupyter kernelspec list\n</code></pre> <p>Example: Add a kernel for a virtual environment</p> <p>As an example, let us create a virtual environment in our <code>$SCRATCH</code> directory called <code>cudapython</code> that installs the CUDA Python  packages.</p> <pre><code>module load anaconda3/2024.02\nmodule load gcc/11.4.0 cuda/12.2.1\n\ncd $SCRATCH\npython -m venv ./cudapython\nsource cudapython/bin/activate\n\npython -m pip install -U cuda-python cuda-parallel cuda-cooperative \npython -m pip install -U cuda-core numba-cuda nvmath-python ipykernel\n\npython -m ipykernel install --user --name cudapython  \\\n                            --display-name \"CUDA Python\" \\\n                            --env PATH $PATH \\\n                            --env LD_LIBRARY_PATH $LD_LIBRARY_PATH\n</code></pre> <p>The above command will create a <code>kernel.json</code> file in <code>~/.local/share/jupyter/kernels/cudapython</code>. You can manually edit this file to edit the paths and environment variables. The new kernel will show up on the jupyter server app on Open OnDemand. Depending on your packages and whether you had to import additional module before installing the package, you may not have to pass the <code>--env PATH</code> and <code>--env LD_LIBRARY_PATH</code> values in the command above. In this examples, the <code>PATH</code> and <code>LD_LIBRARY_PATH</code> variables exported are important because of the <code>cuda-python</code> packages make use of <code>cuda/12.2.1</code> module that we imported.</p>"},{"location":"hpc/openondemand/packages-kernels/#manually-creating-a-new-kernel","title":"Manually creating a new kernel","text":"<p>To add a new kernel to your Jupyter environment, you can also manually create a subdirectory within <code>$HOME/.local/share/jupyter/kernels</code>. Within the subdirectory, you\u2019ll need a configuration file, <code>kernel.json</code>. Each new kernel should have its own subdirectory containing a configuration file.</p> <p>As an example, below is the content of <code>~/.local/share/jupyter/kernels/cudapython/kernel.json</code> file that we just created using the <code>python -m ipykernel install</code> command in the previous section. You can create and/or edit this file as needed. Note that below we have used $SCRATCH instead of the actual path but you will need to provide the full path to your python executable. </p> <pre><code>{\n \"argv\": [\n  \"$SCRATCH/cudapython/bin/python\",\n  \"-m\",\n  \"ipykernel_launcher\",\n  \"-f\",\n  \"{connection_file}\"\n ],\n \"display_name\": \"CUDA Python\",\n \"language\": \"python\",\n \"metadata\": {\n  \"debugger\": true\n },\n \"env\": {\n  \"PATH\": \"/location/to/path1:/location/to/path2\",\n  \"LD_LIBRARY_PATH\": \"/location/to/path1:/location/to/path2\"\n }\n}\n</code></pre> <p>Managing kernels for Jupyter</p> <p>Please review the Jupyter documentation on Managing kernels for Jupyter  for more details regarding the format and contents of this configuration file. In particular, please make sure <code>$PATH, $LD_LIBRARY_PATH, $PYTHONPATH</code>, and all other environment variables that you use in the kernel are properly populated with the correct values.</p>"},{"location":"hpc/openondemand/packages-kernels/#using-a-conda-environment","title":"Using a conda environment","text":"<p>Another approach to adding a new (Python) kernel  to your Jupyter environment is to create a conda environment and add it as a kernel to Jupyter. When in Jupyter, you will then be able to select the name from the kernel list, and it will be using the packages you installed. Follow these steps to do this (replacing $ENV_NAME with the name you want to give your conda environment): </p> <pre><code>module load anaconda3\nconda create --name=$ENV_NAME ipykernel\nconda activate $ENV_NAME\npython -m ipykernel install --user --name $ENV_NAME\n</code></pre> <p>From example, below we give an example of a custom python kernel in a conda environment that uses <code>python=3.12</code> and install <code>numpy=2.0.0</code> from <code>conda-forge</code> channel. </p> <pre><code>module load anaconda3/2024.02\nconda create --name=numpy2test python=3.12 ipykernel\nconda activate numpy2test\nconda config --env -add channels conda-forge\nconda install numpy=2.0.0\npython -m ipykernel install --user --name numpy2 --display-name=\"Numpy v2 (Python 3.12)\"\n</code></pre> <p>Now you can choose the kernel you just created from the kernel list in your Jupyter environment on Open OnDemand. </p>"},{"location":"hpc/openondemand/packages-kernels/#using-apptainer-images","title":"Using apptainer images","text":"<p>It is also possible to create custom kernels using container images. For example, if you would like to use the NVIDIA RAPIDS docker image, first you need to convert it to an apptainer <code>sif</code> file. This can be done using <code>apptainer pull</code> command in your <code>scratch</code> directory as:</p> <pre><code>export APPTAINER_CACHEDIR=$SCRATCH\nexport APPTAINER_TMPDIR=$SCRATCH\n\ncd $SCRATCH\n\napptainer pull docker://nvcr.io/nvidia/rapidsai/notebooks:25.04-cuda12.8-py3.12\n</code></pre> <p>You can then manually create a <code>kernel.json</code> file in <code>~/.local/share/jupyter/kernels/rapids</code> with the following content.</p> <pre><code>{\n    \"argv\": [\n     \"apptainer\",\n     \"exec\",\n     \"--nv\",\n     \"/path/to/notebooks_25.04-cuda11.8-py3.12.sif\",\n     \"python\",\n     \"-m\",\n     \"ipykernel_launcher\",\n     \"-f\",\n     \"{connection_file}\"\n    ],\n    \"display_name\": \"RAPIDS 25.04 Kernel\",\n    \"language\": \"python\",\n    \"metadata\": {\n     \"debugger\": true\n    }\n}\n</code></pre>"},{"location":"hpc/running/gnu-parallel/","title":"GNU Parallel","text":"<p>GNU Parallel is a shell tool for executing jobs in parallel on one or more computers. It\u2019s a helpful tool for automating the parallelization of multiple (often serial) jobs, in particular allowing one to group jobs into a single SLURM submission to take advantage of the multiple cores on a given Lawrencium node.</p> <p>A job can be a single core serial task, multi-core or MPI application. A job can also be a command that reads from a pipe. The typical input is a list of parameters required for each task. GNU parallel can then split the input and pipe it into commands in parallel. GNU parallel makes sure output from the commands is the same output as you would get had you run the commands sequentially, and output names can be easily correlated to input file names for easy post-data processing. This makes it possible to use output from GNU parallel as input for other programs.</p> <p>Below we\u2019ll show basic usage of GNU parallel and then provide an extended example illustrating submission of a job that uses GNU parallel.</p> <p>For full documentation see the GNU parallel man page  and GNU parallel tutorial  .</p> <p>Loading GNU parallel on Lawrencium</p> <p>GNU Parallel is available as a module on Lawrencium. To load GNU Parallel:</p> <pre><code>module load parallel\n</code></pre>"},{"location":"hpc/running/gnu-parallel/#basic-usage","title":"Basic Usage","text":"<p>To motivate usage of GNU parallel, consider how you might automate running multiple individual tasks using a simple bash for loop. In this case, our example command involves copying a file. We will copy <code>file1.in</code> to <code>file1.out</code>, <code>file2.in</code> to <code>file2.out</code>, etc.</p> <pre><code>for (( i=1; i &lt;= 3; i++ )); do\ncp file${i}.in file${i}.out\ndone\n</code></pre> <p>That\u2019s fine, but it won\u2019t run the tasks in parallel. Let\u2019s use GNU parallel to do it in parallel:</p> <pre><code>parallel -j 2 cp file{}.in file{}.out ::: 1 2 3\nls file*out\n# file1.out file2.out file3.out\n</code></pre> <p>Based on <code>-j</code>, that will use two cores to process the three tasks, staring the third task when a core becomes free from having finished either the first or second task. The <code>:::</code> syntax separates the input values <code>1 2 3</code> from the command being run. Each input value is used in place of <code>{}</code> and <code>cp</code> command is run.</p>"},{"location":"hpc/running/gnu-parallel/#extended-example","title":"Extended example","text":"<p>Here we\u2019ll put it all together (and include even more useful syntax) to parallelize use of the bioinformatics software BLAST across multiple biological input sequences. Below are three sample files, including a SLURM job submission script where GNU parallel launches parallel tasks, a bash file to run a serial task and a task list.</p> <p>1) Job submission script:</p> <p>e.g., blast.slurm, where gnu-parallel flags are setup and independent tasks are launched in parallel: <pre><code>#!/bin/bash\n#SBATCH --job-name=job-name\n#SBATCH --account=account_name\n#SBATCH --partition=partition_name\n#SBATCH --nodes=2.\n#SBATCH --time=2:00:00\n## Command(s) to run (example):\n#\nmodule load  bio/blast-plus parallel\nexport WDIR=/your/desired/path\ncd $WDIR\nexport JOBS_PER_NODE=$SLURM_CPUS_ON_NODE\n#\n## when each task is multi-threaded, say NTHREADS=2, then JOBS_PER_NODE should be revised as below\n## JOBS_PER_NODE=$(( $SLURM_CPUS_ON_NODE  / NTHREADS ))\n## when memory footprint is large, JOBS_PER_NODE needs to be set ; $SLURM_CPUS_ON_NODE\n#\necho $SLURM_JOB_NODELIST |sed s/\\,/\\\\n/g; hostfile\n## when GNU parallel can't detect core# of remote nodes, say --progress/--eta,\n## core# should be prepended to hostnames. e.g. 32/n0149.savio3\n## echo $SLURM_JOB_NODELIST |sed s/\\,/\\\\n/g |awk -v cores=$SLURM_CPUS_ON_NODE '{print cores\"/\"$1}'hostfile&lt;\n#\nparallel --jobs $JOBS_PER_NODE --slf hostfile --wd $WDIR --joblog task.log --resume --progress \\\n--colsep ' ' -a task.lst sh run-blast.sh {} output/{/.}.blst $NTHREADS \n</code></pre></p> <ul> <li><code>-a</code>: task list as input to GNU parallel</li> <li><code>\u2013sshloginfile/-slf</code>: compute node list</li> <li><code>\u2013{}</code> : take values from the task list, one line at a time as parameters to the application/serial task (e.g. run-blast.sh)</li> <li>`\u2013{/.}``:remove path and file extension</li> <li><code>\u2013output/{/.}</code>: specify output/file.blst as the blast result;</li> <li><code>\u2013colsep</code>: used as column separator, such as comma, tab, space for the input task list</li> <li><code>\u2013jobs</code>: number of tasks per node</li> <li><code>\u2013wd</code>: landing work dir on compute nodes, default is $HOME</li> <li><code>\u2013joblog</code>: keep track of completed tasks</li> <li><code>\u2013resume</code>: used as a checkpoint, allow jobs to resume</li> <li><code>\u2013progress</code>: display job progress</li> <li><code>\u2013eta</code>: estimated time to finish</li> <li><code>\u2013load</code>: threshold for CPU load, e.g. 80%</li> <li><code>\u2013noswap</code>: new jobs won\u2019t be started if a node is under heavy memory load</li> <li><code>\u2013memfree</code>: check if there is enough free memory, e.g. 2G</li> <li><code>\u2013dray-run</code>: display commands to run without execution</li> </ul> <p>Note: <code>\u2013log logfile</code> pairs with the <code>resume</code> option for production runs. A unique name of logfile is recommended, such as <code>$SLURM_JOB_NAME.log</code> Otherwise, job rerun will not start when the same logfile exists</p> <p>2) Serial bash script:</p> <p><pre><code>#!/bin/bash\nmodule load  bio/blast-plus parallel\n\nblastp -query $1 -db ../blast/db/img_v400_PROT.00 -out $2  -outfmt 7 -max_target_seqs 10 -num_threads $3\n</code></pre> where $1, $2 and $3 are the three parameters required for each serial task</p> <p>3)  Task list: list of parameters for tasks in the format of one line for one task. The parameters required for each task must to be on the same line separated by an eliminators:</p> <pre><code>[user@n0002 ~] cat task.lst\n../blast/data/protein1.faa\n../blast/data/protein2.faa\n</code></pre> <p>In this example, although each task takes three parameters (run-blast.sh), only one parameter is provided in the task list task.lst. The 2nd parameter, which specifies the output, is correlated to output/{/.}.blst in blast.slurm. And the third parameter num_threads is fixed. However, If core# varies from task to task, task.lst could be revised as:</p> <pre><code>[user@n0002 ~] cat task.lst\n../blast/data/protein1.faa 2\n../blast/data/protein2.faa 4\n</code></pre> <p>For best practice, test your code on an interactive node before submitting jobs to clusters.</p> <p>In addition: task list can a sequence of commands, such as:</p> <pre><code>[user@n0002 ~] cat commands.lst\necho \u201chost = \u201d \u2018`hostname`\u2019\nsh -c \u201cecho today date = ; date\u201d\nsh -c \u201cecho today date = ; date\u201d\n</code></pre> <pre><code>[user@n0002 ~] parallel -j 2 &lt; commands.lst\nhost =  n0148.savio3\ntoday date = Sat Apr 18 14:07:33 PDT 2020\n</code></pre>"},{"location":"hpc/running/gnu-parallel/#useful-external-links","title":"Useful external links","text":"<ul> <li>GNU Parallel man page </li> <li>GPU Parallel tutorial </li> </ul>"},{"location":"hpc/running/monitor-jobs/","title":"Monitor Jobs","text":""},{"location":"hpc/running/monitor-jobs/#monitoring-the-status-of-running-batch-jobs","title":"Monitoring the status of running batch jobs","text":"<p>To monitor a running job, you need to know the SLURM job ID of that job, which can be obtained by running</p> <pre><code>squeue -u $USER\n</code></pre>"},{"location":"hpc/running/monitor-jobs/#monitoring-the-job-from-a-login-node","title":"Monitoring the job from a login node","text":"<p>If you suspect your job is not running properly, or you simply want to understand how much memory or how much CPU the job is actually using on the compute nodes, Savio provides a script \u201cwwall\u201d to check that.</p> <p>The following provides a snapshot of node status that the job is running on:</p> <p><pre><code>wwall -j $your_job_id\n</code></pre> while</p> <pre><code>wwall -j $your_job_id -t\n</code></pre> <p>provides a text-based user interface (TUI) to monitor the node status when the job progresses. To exit the TUI, enter \u201cq\u201d to quit out of the interface and be returned to the command line.</p> <p>You can also see a \u201ctop\u201d-like summary for all nodes by running wwtop from a login node. You can use the page up and down keys to scroll through the nodes to find the node(s) your job is using. All CPU percentages are relative to the total number of cores on the node, so 100% usage would mean that all of the cores are being fully used.</p>"},{"location":"hpc/running/monitor-jobs/#monitoring-the-job-by-logging-into-the-compute-node","title":"Monitoring the job by logging into the compute node","text":"<p>Alternatively, you can login to the node your job is running on as follows:</p> <pre><code>srun \u2013jobid=$your_job_id \u2013pty /bin/bash\n</code></pre> <p>This runs a shell in the context of your existing job. Once on the node, you can run top, htop, ps, or other tools.</p> <p>If you\u2019re running a multi-node job, the commands above will get you onto the first node, from which you can ssh to the other nodes if desired. You can determine the other nodes based on the <code>SLURM_NODELIST</code> environment variable.</p>"},{"location":"hpc/running/monitor-jobs/#checking-finished-jobs","title":"Checking finished jobs","text":"<p>First of all, you should look for the SLURM output and error files that may be created in the directory from which you submitted the job. Unless you have specified your own names for these files they will be names slurm-.out and slurm-.err.</p> <p>After a job has completed (or been terminated/cancelled), you can review the maximum memory used via the sacct command.</p> <p><pre><code>sacct -j \u2013format=JobID,JobName,MaxRSS,Elapsed \n</code></pre> MaxRSS will show the maximum amount of memory that the job used in kilobytes.</p> <p>You can check all the jobs that you ran within a time window as follows</p> <pre><code>sacct -u \u2013starttime=2019-09-27 \u2013endtime=2019-10-04 \\\n\u2013format JobID,JobName,Partition,Account,AllocCPUS,State,ExitCode,Start,End,NodeList\n</code></pre> <p>Please see <code>man sacct</code> for a list of the output columns you can request, as well as the SLURM documentation for the <code>sacct</code> command.</p>"},{"location":"hpc/running/script-examples/","title":"Example Scripts","text":"<p>Here we show some example job scripts that allow for various kinds of parallelization such as: jobs that use fewer cores than available on a node, GPU jobs, low-priority condo jobs, and long-running PCA jobs.</p> <p>Please refer to Slurm Association on how to use the command <code>sacctmgr</code> to obtain details of accounts, partitions, and quality of service (qos) that are needed in a slurm script.</p>"},{"location":"hpc/running/script-examples/#example-set-1","title":"Example Set 1","text":"Simple Serial JobSimple Multi-Core JobSerial Tasks in Parallel Job <pre><code>#!/bin/bash\n#SBATCH --job-name=test\n#SBATCH --partition=partition_name\n#SBATCH --qos=qos_name\n#SBATCH --account=account_name\n#SBATCH --time=0:0:30\n## Run command\n./a.out\n</code></pre> <pre><code>#!/bin/bash\n#SBATCH --job-name=test\n#SBATCH --account=account_name\n#SBATCH --partition=partition_name\n#SBATCH --qos=qos_name\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=20\n#SBATCH --cpus-per-task=1\n#\n# Wall clock limit:\n#SBATCH --time=00:00:30\n#\n## Command(s) to run (example):\n./a.out\n</code></pre> <pre><code>#!/bin/bash\n#SBATCH --job-name=job-name\n#SBATCH --account=account_name\n#SBATCH --partition=partition_name\n#SBATCH --qos=qos_name\n#SBATCH --nodes=2\n#SBATCH --cpus-per-task=2\n#SBATCH --time=2:00:00\n#\n## Command(s) to run (example):\nmodule load bio/blast/2.6.0\nmodule load gnu-parallel/2019.03.22\n\nexport WDIR=/your/desired/path\ncd $WDIR\n\n# set number of jobs based on number of cores available and number of threads per job\nexport JOBS_PER_NODE=$(( $SLURM_CPUS_ON_NODE / $SLURM_CPUS_PER_TASK ))\n#\necho $SLURM_JOB_NODELIST |sed s/\\,/\\\\n/g &gt; hostfile\n#\nparallel --jobs $JOBS_PER_NODE --slf hostfile --wd $WDIR --joblog task.log --resume --progress -a task.lst sh run-blast.sh {} output/{/.}.blst $SLURM_CPUS_PER_TASK\n</code></pre>"},{"location":"hpc/running/script-examples/#example-set-2","title":"Example Set 2","text":"Threaded/OpenMP JobMPI Job 1MPI Job 2Hybrid OpenMP+MPI Job <pre><code>#!/bin/bash\n#SBATCH --job-name=test\n#SBATCH --account=account_name\n#SBATCH --partition=partition_name\n#SBATCH --qos=qos_name\n#SBATCH --nodes=1\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=4\n#\n#SBATCH --time=00:00:30\n## Command(s) to run (example):\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n./a.out\n</code></pre> <pre><code>#!/bin/bash\n#SBATCH --job-name=test\n#SBATCH --account=account_name\n#SBATCH --partition=partition_name\n#SBATCH --qos=qos_name\n#SBATCH --ntasks=40     (1)\n#\n# Processors per task:\n#SBATCH --cpus-per-task=1\n#SBATCH --time=00:00:30\n#\n## Command(s) to run (example):\nmodule load gcc openmpi\nmpirun ./a.out\n</code></pre> <ol> <li>Number of MPI tasks needed</li> </ol> <pre><code>#!/bin/bash\n#SBATCH --job-name=test\n#SBATCH --account=account_name\n#SBATCH --partition=partition_name\n#SBATCH --qos=qos_name\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=20\n#SBATCH --cpus-per-task=1\n#SBATCH --time=00:00:30\n#\n## Command(s) to run (example):\nmodule load gcc openmpi\nmpirun ./a.out\n</code></pre> <pre><code>#!/bin/bash\n#SBATCH --job-name=test\n#SBATCH --account=account_name\n#SBATCH --partition=partition_name\n#SBATCH --qos=qos_name\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=4\n#SBATCH --cpus-per-task=5    (1)\n#SBATCH --time=00:00:30      (2)\n#\n## Command(s) to run (example):\nmodule load gcc openmpi\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\nmpirun ./a.out\n</code></pre> <ol> <li>Processors per task needed</li> <li>Wall clock limit</li> </ol>"},{"location":"hpc/running/script-examples/#gpu-job","title":"GPU Job","text":"<p><code>es1</code> partition consists of GPU nodes with three generations of NVIDIA GPU cards (V100, GTX 2080TI, A40). Please take a look at the details on this page. A compute node with different GPU types and numbers can be allocated using slurm in the following way.</p> <ul> <li> <p>General format:  <code>--gres=gpu[type]:count</code></p> </li> <li> <p>The above format can schedule jobs on nodes with V100, GTX 2080TI, or A40 GPU cards. To specify a particular card:</p> <ul> <li> <p>GRTX2080TI: <code>--gres=gpu:GRTX2080TI:1</code> (up to 3 or 4 GPUs)</p> </li> <li> <p>V100 : <code>--gres=gpu:V100:1</code>(up to 2 GPUs)</p> </li> <li> <p>A40: <code>--gres=gpu:A40:1</code> (up to 4 GPUs)</p> </li> <li> <p>H100: <code>--gres=gpu:H100:1</code> (up to 8 GPUs)</p> </li> </ul> </li> </ul> <p>To help the job scheduler effectively manage the use of GPUs, your job submission script must request multiple CPUs (usually two) for each GPU you use. The scheduler will reject jobs submitted that do not request sufficient CPUs for every GPU. This ratio should be one:two.</p> <p>Here\u2019s how to request two CPUs for each GPU: the total of CPUs requested results from multiplying two settings: the number of tasks (<code>--ntasks=</code>) and CPUs per task (<code>--cpus-per-task=</code>).</p> <p>For instance, in the above example, one GPU was requested via <code>--gres=gpu:1</code>, and the required total of two CPUs was thus requested via the combination of <code>--ntasks=1</code> and --cpus-per-task=2 . Similarly, if your job script requests four GPUs via <code>--gres=gpu:4</code>, and uses <code>--ntasks=8</code>, it should also include <code>--cpus-per-task=1</code> to request the required total of eight CPUs.</p> <p>Note that in the <code>--gres=gpu:n</code> specification, <code>n</code> must be between 1 and the number of GPUs on a single node (which is provided here for the various GPU types). This is because the feature is associated with how many GPUs per node to request.</p> <p>Examples:</p> <ul> <li>Request one V100 card: <code>--cpus-per-task=4 --gres=gpu:V100:1 --ntasks 1</code></li> <li>Request two A40 cards: <code>--cpus-per-task=16 --gres=gpu:A40:2 --ntasks 2</code></li> <li>Request three H100 cards: <code>--cpus-per-task=14 --gres=gpu:H100:3 --ntasks 3</code></li> </ul> <pre><code>#!/bin/bash\n#SBATCH --job-name=test\n#SBATCH --account=account_name\n#SBATCH --partition=es1\n#SBATCH --qos=es_normal\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#\n# Processors per task (please always specify the total number of processors twice the number of GPUs):\n#SBATCH --cpus-per-task=2\n#\n#Number of GPUs, this can be in the format of \"gpu:[1-4]\", or \"gpu:V100:[1-4] with the type included\n#SBATCH --gres=gpu:1\n#\n# Wall clock limit:\n#SBATCH --time=1:00:00\n#\n## Command(s) to run (example):\n./a.out\n</code></pre>"},{"location":"hpc/running/slurm-overview/","title":"Slurm Overview","text":"<p>SLURM is a resource manager and job scheduling system developed by SchedMD. The trackable resources (TRES)   include Nodes, CPUs, Memory and  Generic Resources (GRES). </p> <p>Slurm has three key functions: </p> <ol> <li>Allocate resources exclusively/non-exclusive to nodes, </li> <li>start/execute and monitor the resources on a node, and </li> <li>arbitrate pending and queued work.  </li> </ol> <p>Nodes are grouped together within a partition.  The partitions can also be considered   as job queues and each of which has a set of constraints such as job size limit, time limit, default memory limits,   and the number of nodes, etc. Submitting a job to the system requires you to specify a partition, an account, a   Quality of Service (QoS), the number of nodes, wallclock time limits and optional memory (default will be used if   not specified). Jobs within a partition will then be allocated to nodes based on the scheduling policy, until all   resources within a partition are exhausted.</p> <p>There are several basic commands you will need to know to submit jobs, cancel jobs, and check status. These are:</p> <ul> <li><code>sbatch</code> \u2013 submit a job to the batch queue system, e.g., <code>sbatch myjob.sh</code></li> <li> <p><code>squeue</code> \u2013 check the current jobs in the batch queue system, e.g., <code>squeue</code></p> <p>By default, <code>squeue</code> command will list all the jobs in the queue system. To list only the jobs pertaining to your username $USER, use the command:</p> <pre><code>squeue -u $USER\n</code></pre> </li> <li> <p><code>sinfo</code> \u2013 view the current status of the queues, e.g., <code>sinfo</code></p> </li> <li><code>scancel</code> \u2013 cancel a job, e.g., <code>scancel 1234567</code></li> <li><code>srun</code> \u2013 to run interactive jobs</li> </ul> <p>Interactive job using <code>srun</code></p> <p>You can use <code>srun</code> to request and run an interactive job. The following example (please change the <code>account_name</code> and the <code>partition_name</code> according to your needs) requests a <code>lr4</code> node for 1 hour.</p> <pre><code>srun -p lr4 -A account_name -q lr_normal -N 1 -t 1:00:00 --pty bash\n</code></pre> <p>The prompt will change to indicate that you are on the compute node allocated for the interactive job once the interactive job starts:</p> <pre><code>srun: job 7566529 queued and waiting for resources\nsrun: job 7566529 has been allocated resources\n[user@n0105 ~]$ \n</code></pre> <p>If you are done working on the interactive job before the allocated time, you can release the resource by using <code>exit</code> on the interactive node. </p>"},{"location":"hpc/running/slurm-overview/#slurm-association","title":"Slurm Association","text":"<p>A Slurm job submission script includes a list of SLURM directives (or commands) to tell the job scheduler what to do.  This information, such as user account, cluster partition and QoS (Quality of Service), have to be paired correctly in your job submission scripts. The Slurm command \u2018sacctmgr\u2018 provide accounts, partitions and the QoSs that available to you as a user. <pre><code>sacctmgr show association -p user=$USER\n</code></pre></p> <p>The command returns the output for a hypothetical example user <code>userA</code>. To be specific, <code>userA</code> has access to a PI Computing Allowance <code>pc_acctB</code>, departmental cluster nano and the condo account <code>lr_acctA</code> with respect to different partitions. Each line of this output indicates a specific combination of an account, a partition, and QoSes that you can use in a job script file, when submitting any individual batch job:</p> <pre><code>Cluster|Account|User|Partition|Share|\u2026|QOS|\u2026\nperceus-00|pc_acctB|userA|ood_inter|1|||||||||||||lr_interactive|||\nperceus-00|pc_acctB|userA|cm1|1|||||||||||||cm1_debug,cm1_normal|||\nperceus-00|pc_acctB|userA|lr6|1|||||||||||||lr_debug,lr_normal|||\nperceus-00|pc_acctB|userA|lr_bigmem|1|||||||||||||lr_normal|||\nperceus-00|pc_acctB|userA|lr5|1|||||||||||||lr_debug,lr_normal|||\nperceus-00|pc_acctB|userA|lr4|1|||||||||||||lr_debug,lr_normal|||\nperceus-00|pc_acctB|userA|lr3|1|||||||||||||lr_debug,lr_normal|||\nperceus-00|pc_acctB|userA|cf1|1|||||||||||||cf_debug,cf_normal|||\nperceus-00|pc_acctB|userA|es1|1|||||||||||||es_debug,es_lowprio,es_normal|||\n</code></pre> <p>The Account, Partition, and QOS indicate which partitions and QoSes you have access to under each of your account(s).</p>"},{"location":"hpc/software/module-management/","title":"Module Management","text":"<p>The Software Module Farm (SMF) is managed by the Lmod  Environment Module system to set the appropriate environment variables in your shell needed to make use of the individual software packages. </p> <p>Environment Modules are used to manage users\u2019 runtime environments dynamically. This is accomplished by loading and unloading modulefiles which contain the application specific information for setting a user\u2019s environment, primarily the shell environment variables, such as <code>PATH</code>, <code>LD_LIBRARY_PATH</code>, etc. Modules are useful in managing different applications, and different versions of the same application in a cluster environment.</p> <p>The following commands are some frequently useful commands to manipulate modules in your environment:</p> <pre><code>module load SOFTWARE    # Load the module \u201cSOFTWARE\u201d\nmodule unload SOFTWARE  # Unload the module \u201cSOFTWARE\u201d\nmodule available        # List all modules available for loading\nmodule list             # List all modules currently loaded\n</code></pre>"},{"location":"hpc/software/module-management/#finding-modules","title":"Finding Modules","text":"<p>module spider</p> <p><code>module spider SOFTWARE</code> </p> <p>is a useful module command that lists the module(s) named <code>SOFTWARE</code> and information on additional modules that you may need to load before <code>SOFTWARE</code> is available to load</p> <p>For example: </p> <pre><code>[user@n0000 ~]$ module spider hdf5\n\n-------------------------------------------------------------------------\n  hdf5: hdf5/1.14.3\n-------------------------------------------------------------------------\n\nYou will need to load all module(s) on any one of the lines below before \nthe \"hdf5/1.14.3\" module is available to load.\n\n      gcc/10.5.0  openmpi/4.1.3\n      gcc/10.5.0  openmpi/4.1.6\n      gcc/11.4.0  openmpi/4.1.3\n      gcc/11.4.0  openmpi/4.1.6\n      intel-oneapi-compilers/2023.1.0  intel-oneapi-mpi/2021.10.0\n\n    Help:\n      HDF5 is a data model, library, and file format for storing and \n      managing data. It supports an unlimited variety of datatypes, \n      and is designed for flexible and efficient I/O and for high volume \n      and complex data.\n</code></pre> <p>This means that you will need to load the appropriate compiler + mpi library combination before being able to load the corresponding hdf5 module. For example, you can do the following:</p> <pre><code>module load gcc/11.4.0\nmodule load openmpi/4.1.6\nmodule load hdf5\n</code></pre>"},{"location":"hpc/software/module-management/#environment-modules-usage-examples","title":"Environment Modules Usage Examples","text":"<p>There are some basic commands that users will need to know to work with the Environment Modules system, which all starts with the primary \u201cmodule\u201d command, and followed by a subcommand listed below (\u201c|\u201d means \u201cor\u201d, e.g., \u201cmodule add\u201d and \u201cmodule load\u201d are equivalent). For detail usage instruction of the \u201cmodule\u201d command please run \u201cman module\u201d.</p> <ul> <li><code>module avail</code> \u2013 List all available modulefiles in the current <code>MODULEPATH</code>.</li> <li><code>module list</code> \u2013 List loaded modules.</li> <li><code>module add|load modulefile \u2026</code> \u2013 Load modulefile(s) into the shell environment.</li> <li><code>module rm|unload modulefile</code> \u2026 \u2013 Remove modulefile(s) from the shell environment.</li> <li><code>module swap|switch [modulefile1] modulefile2</code> \u2013 Switch loaded modulefile1 with modulefile2.</li> <li><code>module show|display modulefile \u2026</code> \u2013 Display information about one or more modulefiles.</li> <li><code>module whatis [modulefile \u2026]</code> \u2013 Display the information about the modulefile(s).</li> <li><code>module purge</code> \u2013 Unload all loaded modulefiles.</li> </ul> <p>Below we demonstrate how to use these commands. Depending on which system you have access to and when you are reading this instruction, what you see here could be different from the actual output from the system that you work on. </p> <p>module avail</p> <pre><code>[user@n0000.scs00 ~]$ module avail\n------------------------- /global/software/rocky-8.x86_64/modfiles/compilers ---------------------\n  gcc/10.5.0    gcc/11.4.0 (D)    intel-oneapi-compilers/2023.1.0    llvm/17.0.4    nvhpc/23.9\n\n------------------------- /global/software/rocky-8.x86_64/modfiles/tools -------------------------\n  automake/1.16.5       ffmpeg/6.0              lmdb/0.9.31          proj/9.2.1         tcl/8.6.12\n  awscli/1.29.41        gdal/3.7.3              m4/1.4.19            protobuf/3.24.3    tmux/3.3a\n  bazel/6.1.1           glog/0.6.0              matlab/r2022a        qt/5.15.11         unixodbc/2.3.4\n  cmake/3.27.7          gmake/4.4.1             mercurial/6.4.5      rclone/1.63.1      valgrind/3.20.0\n  code-server/4.12.0    gurobi/10.0.0           nano/7.2             snappy/1.1.10      vim/9.0.0045\n  eigen/3.4.0           imagemagick/7.1.1-11    ninja/1.11.1         spack/0.20.1\n  emacs/29.1            leveldb/1.23            parallel/20220522    swig/4.1.1\n\n------------------------- /global/software/rocky-8.x86_64/modfiles/langs -------------------------\n  anaconda3/2024.02-1-11.4    openjdk/11.0.20.1_1-gcc-11.4.0        r/4.3.0-gcc-11.4.0\n</code></pre> <p>module list</p> <pre><code>[user@n0000 ~]$ module list\nNo modules loaded\n</code></pre> <p>module load</p> <pre><code>[user@n0000 ~]$ module load gcc\n[user@n0000 ~]$ module load openmpi\n[user@n0000 ~]$ module list\n\nCurrently Loaded Modules:\n  1) gcc/11.4.0   2) ucx/1.14.1   3) openmpi/4.1.6\n</code></pre> <p>On systems in which a hierarchical structure is used, some of modulefiles will only be available after the root modulefile is loaded. The Lawrencium cluster uses a hierarchical structure for several packages that depend on a particular compiler and/or MPI package. For example, after loading <code>gcc/11.4.0</code> and <code>openmpi/4.1.6</code> in the example above, <code>module avail</code> will show new packages that can now be loaded:</p> <p>hierarchical structure</p> <pre><code>[user@n0000 ~]$ module avail\n\n------------- /global/software/rocky-8.x86_64/modfiles/openmpi/4.1.6-4xq5u5r/gcc/11.4.0 ------------\n  boost/1.83.0      hmmer/3.4                        ncl/6.6.2         netcdf-fortran/4.6.1\n  fftw/3.3.10       intel-oneapi-mkl/2023.2.0 (D)    nco/5.1.6         netlib-lapack/3.11.0   (D)\n  gromacs/2023.3    lammps/20230802                  ncview/2.1.9      netlib-scalapack/2.2.0\n  hdf5/1.14.3       mumps/5.5.1                      netcdf-c/4.9.2    petsc/3.20.1\n\n------------------------ /global/software/rocky-8.x86_64/modfiles/gcc/11.4.0 -----------------------\n  antlr/2.7.7                gsl/2.7.1                     openmpi/4.1.6   (L,D)\n  blast-plus/2.14.1          idba/1.1.3                    picard/2.25.7\n  bowtie2/2.5.1              intel-oneapi-mkl/2023.2.0     prodigal/2.6.3\n  cuda/11.8.0                intel-oneapi-tbb/2021.10.0    samtools/1.17\n  cuda/12.2.1         (D)    netlib-lapack/3.11.0          ucx/1.14.1      (L)\n  cudnn/8.7.0.84-11.8        openblas/0.3.24               udunits/2.2.28\n  cudnn/8.9.0-12.2.1  (D)    openmpi/4.1.3                 vcftools/0.1.16\n</code></pre> <p>For example, now <code>gromacs</code> can be loaded.</p> <p>Note</p> <pre><code>[sadhikari@n0000 ~]$ module load gromacs\n[sadhikari@n0000 ~]$ module list\n\nCurrently Loaded Modules:\n  1) gcc/11.4.0   3) openmpi/4.1.6   5) intel-oneapi-tbb/2021.10.0   7) netlib-lapack/3.11.0\n  2) ucx/1.14.1   4) fftw/3.3.10     6) intel-oneapi-mkl/2023.2.0    8) gromacs/2023.3\n</code></pre> <p><code>module show</code> command displays information about the module.</p> <p>module show</p> <pre><code>[user@n0000 ~]$ module show fftw\n---------------------------------------------------------------------------------------------------------\n  /global/software/rocky-8.x86_64/modfiles/openmpi/4.1.6-4xq5u5r/gcc/11.4.0/fftw/3.3.10.lua:\n---------------------------------------------------------------------------------------------------------\nwhatis(\"Name : fftw\")\nwhatis(\"Version : 3.3.10\")\nwhatis(\"Target : x86_64\")\nwhatis(\"Short description : FFTW is a C subroutine library for computing the discrete Fourier transform (DFT)\nin one or more dimensions, of arbitrary input size, and of both real and complex data (as well as of even/od\nd data, i.e. the discrete cosine/sine transforms or DCT/DST). We believe that FFTW, which is free software, s\nhould become the FFT library of choice for most applications.\")\nhelp([[Name   : fftw]])\nhelp([[Version: 3.3.10]])\nhelp([[Target : x86_64]])\n]])\nhelp([[FFTW is a C subroutine library for computing the discrete Fourier\ntransform (DFT) in one or more dimensions, of arbitrary input size, and\nof both real and complex data (as well as of even/odd data, i.e. the\ndiscrete cosine/sine transforms or DCT/DST). We believe that FFTW, which\nis free software, should become the FFT library of choice for most\napplications.]])\ndepends_on(\"openmpi/4.1.6\")\nprepend_path(\"PATH\",\"/global/software/rocky-8.x86_64/gcc/linux-rocky8-x86_64/gcc-11.4.0/fftw-3.3.10-cf4npbktu\neip6tnwqf2qstog7on4pyfk/bin\")\nprepend_path(\"MANPATH\",\"/global/software/rocky-8.x86_64/gcc/linux-rocky8-x86_64/gcc-11.4.0/fftw-3.3.10-cf4npb\nktueip6tnwqf2qstog7on4pyfk/share/man\")\nprepend_path(\"PKG_CONFIG_PATH\",\"/global/software/rocky-8.x86_64/gcc/linux-rocky8-x86_64/gcc-11.4.0/fftw-3.3.1\n0-cf4npbktueip6tnwqf2qstog7on4pyfk/lib/pkgconfig\")\nprepend_path(\"CMAKE_PREFIX_PATH\",\"/global/software/rocky-8.x86_64/gcc/linux-rocky8-x86_64/gcc-11.4.0/fftw-3.3\n.10-cf4npbktueip6tnwqf2qstog7on4pyfk/.\")\nappend_path(\"MANPATH\",\"\")\n</code></pre> <p>The <code>module purge</code> command unloads all currently loaded modulefiles.</p> <p>module purge</p> <pre><code>[user@n0000 ~]$ module purge\n[user@n0000 ~]$ module list\nNo modules loaded\n</code></pre>"},{"location":"hpc/software/module-management/#user-generated-modulefiles","title":"User Generated Modulefiles","text":"<p>User generated modulefiles</p> <p>Users can generate their own modulefiles to load user-specific applications. The path of the modulefiles needs to be appended to the <code>MODULEPATH</code> environment variable as follows:</p> <p>1). For bash users, please add the following to ~/.bashrc:</p> <pre><code>export MODULEPATH=$MODULEPATH:/location/to/my/modulefiles\n</code></pre> <p>2). For csh/tcsh users, please add the following to ~/.cshrc:</p> <pre><code>setenv MODULEPATH \u201d$MODULEPATH\u201d:/location/to/my/modulefiles\n</code></pre>"},{"location":"hpc/software/software-module-farm/","title":"Software Module Farm","text":"<p>Software Module Farm (SMF) Service</p> <p>For an overview of the Software Module Farm (SMF) as a service on non-Lawrencium systems, please follow this link . In this page, we will focus on the Software Module Farm (SMF) software packages and their usage on the Lawrencium cluster.</p> <p>The Software Module Farm provides a comprehensive and well-tested suite of software modules for Lawrencium users. Several types of software modules are available:</p> <ol> <li>Tools: Tool modules are built and compiled with the default system <code>gcc</code> compiler. They have no other dependencies. For the current operating system, the <code>gcc</code> system compiler is <code>gcc 8.5.0</code>.</li> <li>Compilers: Other common compilers and newer versions of <code>gcc</code>; for example: <code>gcc 11.4.0</code>. Many applications and libraries not found in the Tools are built with these compilers and can be accessed after loading the corresponding compiler.</li> <li>Languages: Language modules include additional compilers and interpreters for specific languages such as <code>python</code>, <code>R</code> and <code>julia</code>.</li> <li>Applications: Domain specific applications such as biology and machine learning packages.</li> <li>Submodules: Submodules include libraries and packages which depend on a particular compiler or language module. Due to this dependency, submodules will only be visible once the associated language or core compiler module has been loaded. For example, <code>hdf5</code> submodule is only visible once you load <code>gcc</code> and <code>openmpi</code> modules.</li> </ol> <p>See the Module Management page for details on how to use the <code>module</code> command for module management on Lawrencium.</p>"},{"location":"hpc/software/software-module-farm/#software-installation-by-users","title":"Software installation by Users","text":"<p>Users are encouraged to install domain scientific software packages or local software module farms in their home or group space. Users don\u2019t have admin rights, but most software can be installed with the flag <code>--prefix=/dir/to/your/path</code>.</p>"},{"location":"hpc/software/applications/vasp/","title":"VASP on Lawrencium","text":"<p>The Vienna Ab initio Simulation Package (VASP) is a suite for quantum-mechanical molecular dynamics (MD) simulations and electronic structure calculations. VASP is a licensed package and the license is sold on a research group basis. HPCS group has compiled a VASP 6.4.1 version of the package on Lawrencium. License holder users or group of users can get access to package on request. New licensees need to complete the VASP: Access Request form to be added to the linux groups authorized to use VASP. Please, provide the proof of purchase with this request. Please feel free to reach out to us at hpcshelp@lbl.gov if you would like us to update the version of the package. </p> <p>VASP binaries provided on Lawrencium are compiled targeting CPU or GPU partitions. Following guidelines can help users to run vasp calculation.</p>"},{"location":"hpc/software/applications/vasp/#vasp-cpu-binary-intel-compiler","title":"VASP CPU Binary (Intel Compiler)","text":"<p>The <code>vasp/6.4.1-cpu-intel</code> module is compiled using the intel compiler and mpi modules. To load the module:</p> <pre><code>module load intel-oneapi-compilers/2023.1.0\nmodule load intel-oneapi-mpi/2021.10.0\nmodule load vasp/6.4.1-cpu-intel\n</code></pre> <p>Sample VASP CPU slurm script</p> <p>Please modify the <code>account</code>, <code>qos</code>, <code>ntasks</code>, <code>time</code> and other variables in the sample job scripts below appropriately before running your job. <pre><code>#!/bin/bash\n#SBATCH --job-name=\"check\"\n#SBATCH --ntasks=14\n#SBATCH --cpus-per-task=4\n#SBATCH --output=%x.out\n#SBATCH --error=%x.err\n#SBATCH --time=1:00:00\n#SBATCH --partition=lr7\n#SBATCH --account=&lt;account&gt;\n#SBATCH --qos=lr_normal\n\nmodule load intel-oneapi-compilers/2023.1.0\nmodule load intel-oneapi-mpi/2021.10.0\nmodule load vasp/6.4.1-cpu-intel\n\nexport OMP_NUM_THREADS=4\n\nsrun --mpi=pmi2 vasp_std\n</code></pre></p>"},{"location":"hpc/software/applications/vasp/#vasp-gpu-binary-nvhpc-sdk","title":"VASP GPU Binary (NVHPC SDK)","text":"<p>The <code>vasp/6.4.1-gpu</code> module is compiled using NVHPC SDK. To load the module:</p> <pre><code>module load nvhpc/23.11\nmodule load vasp/6.4.1-gpu\n</code></pre> <p>Sample VASP GPU slurm script</p> <p>The following sample script runs VASP on a <code>H100 es1</code> node using 2 <code>H100</code> GPUs.</p> <pre><code>#!/bin/bash\n#SBATCH --job-name=\"rfm_VASPCheck_1401ba9b\"\n#SBATCH --ntasks=2\n#SBATCH --cpus-per-task=14\n#SBATCH --output=vasp_job.out\n#SBATCH --error=vasp_job.err\n#SBATCH --time=1:00:00\n#SBATCH --partition=es1\n#SBATCH --account=&lt;account&gt;\n#SBATCH --qos=es_normal\n#SBATCH --gres=gpu:H100:2\n\nmodule load nvhpc/23.11\nmodule load vasp/6.4.1-gpu\n\nexport PMIX_MCA_psec=native\nexport OMP_NUM_THREADS=1\n\nmpirun -np 2 vasp_std\n</code></pre>"},{"location":"hpc/software/applications/vasp/#compiling-vasp","title":"Compiling VASP","text":"<p>Users can also compile the package on their own in their home or group space. Please reach out to us if you need help setting up makefile for GNU, intel or nvhpc compilers. </p>"},{"location":"hpc/software/compilers/gcc/","title":"GCC Compilers on Lawrencium","text":"<p>Several <code>gcc</code> compiler versions are available on Lawrencium. The default <code>gcc</code> compiler is <code>gcc/11.4.0</code> available through <code>module load gcc</code>. Two other <code>gcc</code> versions are available: <code>gcc/10.5.0</code> and <code>gcc/13.2.0</code>. To load a <code>gcc</code> module other than the default, specify the version; for example:</p> <pre><code>module load gcc/13.2.0\n</code></pre> <p>The C, C++ and fortran compilers in the <code>gcc</code> compiler suite are:</p> <ul> <li>C: <code>gcc</code></li> <li>C++: <code>g++</code></li> <li>Fortran: <code>gfortran</code></li> </ul>"},{"location":"hpc/software/compilers/gcc/#additional-references","title":"Additional References","text":"<ul> <li>GCC 13.2.0 Manual </li> <li>GCC 11.4.0 Manual </li> <li>GCC 10.5.0 Manual </li> <li>GCC Online Documentation </li> </ul>"},{"location":"hpc/software/compilers/intel/","title":"Intel OneAPI Compilers on Lawrencium","text":"<p><code>intel-oneapi-compilers</code> version <code>2023.1.0</code> is available on Lawrencium which consists of both the new LLVM-based oneAPI compilers <code>icx, icpx, ifx</code> and the  intel classic compilers <code>icc, icpc, ifort</code>. The default intel-oneapi-compilers` module can be loaded as:</p> <pre><code>module load intel-oneapi-compilers\n</code></pre>"},{"location":"hpc/software/compilers/intel/#llvm-based-oneapi-compilers","title":"LLVM-based oneAPI Compilers","text":"<p>The version of LLVM-based oneAPI compilers <code>icx, icpx, ifx</code> follow the version of the oneapi package. Some relevant reference pages on the intel documentation website for the <code>2023.1.0</code> version of oneAPI compilers installed on Lawrencium are listed below:</p> <ul> <li>Intel oneAPI DPC++/C++ Compiler Developer Guide and Reference </li> <li>Intel Fortran Compiler Classic and Intel Fortran Compiler Developer Guide and Reference </li> </ul>"},{"location":"hpc/software/compilers/intel/#intel-classic-compilers","title":"Intel Classic Compilers","text":"<p>Version scheme of Intel Classic Compilers</p> <p>The versions of Intel classic compilers in the module <code>intel-oneapi-compilers/2023.1.0</code> is different than <code>2023.1.0</code>. The version of <code>ifort, icc, icpc</code> compilers in the module <code>intel-oneapi-compilers/2023.1.0</code> is <code>2021.9.0</code></p> <p>Some relevant reference pages on the intel documentation website for the <code>2021.9.0</code> version of Intel classic compilers installed on Lawrencium are listed below:</p> <ul> <li>Intel C++ Compiler Classic Developer Guide and Reference </li> <li>Intel Fortran Compiler Classic and Intel Fortran Compiler Developer Guide and Reference </li> </ul>"},{"location":"hpc/software/compilers/intel/#additional-references","title":"Additional References","text":"<ul> <li>Porting Guide for ICC users to DPCPP or ICX </li> <li>Porting Guide for IFORT to IFX </li> </ul>"},{"location":"hpc/software/compilers/nvhpc/","title":"NVHPC: NVIDIA HPC SDK","text":"<p>The NVIDIA HPC SDK version 23.11 is available on Lawrencium. You can load the <code>nvhpc</code> module as:</p> <pre><code>module load nvhpc/23.11\n</code></pre> <p>The <code>nvhpc</code> module consists of the following compilers:</p> <ul> <li>C: <code>nvc</code>,</li> <li>C++: <code>nvc++</code> </li> <li>Fortran: <code>nvfortran</code></li> </ul> <p>The CUDA C and CUDA C++ compiler driver <code>nvcc</code>  is also present in the module.</p>"},{"location":"hpc/software/compilers/nvhpc/#cuda-versions","title":"CUDA Versions","text":"<p><code>nvhpc/23.11</code> includes two CUDA toolkit versions: <code>CUDA 11.8</code> and <code>CUDA 12.3</code>. You can choose a particular version by using the compiler flag <code>-gpu</code>; for example, use  <code>-gpu=cuda11.8</code> to choose <code>CUDA 11.8</code> when compiling a program using <code>nvhpc</code> compilers.</p>"},{"location":"hpc/software/compilers/nvhpc/#target-architecture","title":"Target Architecture","text":"<p>The <code>-tp=&lt;target&gt;</code> flag can be used to specify a target processor when compiling using <code>nvhpc</code> compilers.</p> <p>-fast</p> <p><code>-fast</code> compiler option is useful to choose an optimal set of vectorization options, but can lead to an auto-selection of the <code>-tp</code> option. This might mean that your compiled code may not work on all Lawrencium partitions as Lawrencium includes a wide range of hardware across several partitions.</p>"},{"location":"hpc/software/compilers/nvhpc/#mpi","title":"MPI","text":"<p><code>nvhpc</code> module includes a version of openmpi. MPI wrapper programs such as <code>mpicc</code>, <code>mpicxx</code> and <code>mpifort</code> are available once <code>nvhpc</code> is loaded. </p> <p>Running MPI programs</p> <p>To run MPI jobs compiled with the <code>nvhpc</code> module on Lawrencium, use <code>mpirun</code> instead of <code>srun</code>. </p> <p>Special considerations</p> <p>For some GPU nodes with AMD CPU hosts such as <code>A40</code> nodes, we have found that the following environment variable needs to be set when using <code>nvhpc</code>'s <code>mpirun</code> command to launch MPI jobs:</p> <pre><code>export PMIX_MCA_psec=native\n</code></pre> <p>In addition, <code>--bind-to core</code> which is the default for <code>mpirun</code> might not work; in which case, you can try <code>--bind-to none</code> or <code>--bind-to socket</code>. For example:</p> <pre><code>mpirun -np 2 --bind-to socket ./program\n</code></pre>"},{"location":"hpc/software/compilers/nvhpc/#additional-references","title":"Additional References","text":"<ul> <li>NVIDIA HPC SDK Version 23.11 Documentation </li> <li>NVIDIA HPC SDK Version 23.11 Release Notes </li> </ul>"},{"location":"hpc/software/languages/R/","title":"Using R on Lawrencium","text":"<p>Version <code>4.4.0</code> of R is available to users; the R module can be loaded as: <pre><code>module load r\n</code></pre> Some commonly used r-packages are already installed with the R module available on the system. To view the list of packages already installed, use the following command in the R command prompt (either in your terminal or RStudio session on Open OnDemand):</p> <pre><code>installed.packages()\n</code></pre> <p>Another module <code>r-spatial</code> is available for a standard set of R packages for spatial data: <pre><code>module load r-spatial\n</code></pre></p>"},{"location":"hpc/software/languages/julia/","title":"Using Julia on Lawrencium","text":"<p>Julia is available on Lawrencium as a module.</p> <pre><code>$ module av julia\n\n-------------- /global/software/rocky-8.x86_64/modfiles/langs ---------------\n   julia/1.10.2-11.4\n</code></pre> <pre><code>$ module load julia\n$ julia\n               _\n   _       _ _(_)_     |  Documentation: https://docs.julialang.org\n  (_)     | (_) (_)    |\n   _ _   _| |_  __ _   |  Type \"?\" for help, \"]?\" for Pkg help.\n  | | | | | | |/ _` |  |\n  | | |_| | | | (_| |  |  Version 1.10.2 (2024-03-01)\n _/ |\\__'_|_|_|\\__'_|  |  Official https://julialang.org/ release\n|__/                   |\n\njulia&gt; \n</code></pre>"},{"location":"hpc/software/languages/julia/#using-julia-through-jupyter-on-open-ondemand","title":"Using Julia through Jupyter on Open OnDemand","text":"<p>A Julia kernel has been added to Jupyter on Open OnDemand that allows you to use Julia on the Jupyter server on Open OnDemand.</p>"},{"location":"hpc/software/languages/python/","title":"Using Python on Lawrencium","text":""},{"location":"hpc/software/languages/python/#python-packages","title":"Python packages","text":"<p>Python 2</p> <p>The <code>rocky-8</code> operating system in Lawrencium has installation of <code>Python 3.6</code> and <code>Python 2.7</code>. To use these,  use the command <code>python3</code> and <code>python2</code> respectively without loading other python modules.</p> <p>Several Python modules are available on the Lawrencium software module farm. There are two basic (with only a few additional site-packages) python modules provided. To list these python modules:</p> <pre><code>$ module av python\n\n---------- /global/software/rocky-8.x86_64/modfiles/langs ----------\n   python/3.10.12-gcc-11.4.0    python/3.11.6-gcc-11.4.0 (D)\n\n$ module load python/3.10.12\n$ python\nPython 3.10.12 (main, Mar 22 2024, 00:44:12) [GCC 11.4.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt; \n</code></pre> <p>To load one of these modules use: <code>module load python/3.10.12</code> or <code>module load python/3.11.6</code>. Additional site-packages installed in these python modules are: <code>numpy</code>, <code>scipy</code>, <code>matplotlib</code>, <code>mpi4py</code>, <code>h5py</code>,<code>netCDF4</code>, <code>pandas</code>, <code>geopandas</code>, <code>ipython</code> and <code>pyproj</code>.</p> <p>User installation of python packages</p> <p>You can use pip to install or upgrade packages.</p> <p><pre><code>python -m pip install --user $PACKAGENAME \n</code></pre> to install a python package to <code>~/.local</code> directory. The package libraries are usually installed in a sub-directory for each python version; for example: <code>~/.local/lib/python3.10/site-packages/</code>.</p> <p>Choosing python modules</p> <p>Please note that the linear algebra backend for <code>numpy</code> in these two python modules (<code>python/3.11.6</code> and <code>python/3.10.12</code>) is the openBLAS library whereas the Anaconda distributions (<code>anaconda3/2024.02</code> and <code>anaconda3/2024.10</code>) use the Intel MKL library. Some linear algebra operations can be faster using <code>numpy</code> through the <code>anaconda3</code> module.</p>"},{"location":"hpc/software/languages/python/#anaconda-environment","title":"Anaconda environment","text":"<p>We also provide <code>anaconda3/2024.02</code> and <code>anaconda3/2024.10</code> python environments that have many popular scientific and numerical python libraries pre-installed. </p> <p>Examples of loading anaconda3</p> anaconda3/2024.02anaconda3/2024.10 <pre><code>$ module load anaconda3/2024.02\n$ python\nPython 3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n&gt;&gt;&gt;\n</code></pre> <pre><code>$ module load anaconda3/2024.10\n$ python\nPython 3.12.7 | packaged by Anaconda, Inc. | (main, Oct  4 2024, 13:27:36) [GCC 11.2.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n</code></pre> <p>Several Jupyter kernels are available to access <code>tensorflow</code> and <code>pytorch</code> conda environments from the Jupyter server on Open OnDemand. Click here for more information on installing python packages and jupyter kernels for use on the Jupyter server on Open OnDemand.</p>"},{"location":"hpc/software/languages/python/#intel-distribution-of-python","title":"Intel Distribution of Python","text":"<p>Additionally the Intel Distribution of Python (Python 3.9)  is available, and can be loaded as:</p> <pre><code>module load intelpython/3.9.19\n</code></pre> <p>When you load <code>intelpython</code>, <code>intel-oneapi-compilers</code> and <code>intel-oneapi-mpi</code> are also loaded because we have added <code>mpi4py</code> package linked to Intel MPI library to the Intel Distribution of Python.</p>"},{"location":"hpc/software/languages/python/#using-dask","title":"Using Dask","text":"<p>Dask  is available both in the <code>anaconda3</code> and <code>intelpython</code> modules. Dask can be useful when you are working with large datasets that don't fit in the memory of a single machine. Dask implements lazy evaluation, task scheduling and data chunking that makes it useful when performing analysis on large datasets.</p> <p>Dask JupyterLab Extension</p> <p>Dask JupyterLab Extension can be used to manage Dask clusters and monitor it through various dashboard plots in JupyterLab panes.</p> <p>To install dask-labextension once you have a python module loaded:</p> <pre><code>python -m pip install dask-labextension\n</code></pre>"},{"location":"hpc/software/libraries/cuda/","title":"CUDA Toolkit","text":"<p>The NVIDIA CUDA Toolkit, consisting of Nvidia-GPU-accelerated libraries, C/C++ compiler and various related tools, is available under <code>gcc</code> compiler tree. <code>cuda/11.8.0</code> and <code>cuda/12.2.1</code> are available after loading a <code>gcc</code> module. For example:</p> <pre><code>module load gcc/11.4.0\nmodule load cuda/12.2.1\n</code></pre> <p>loads CUDA Toolkit version 12.2.1. The environment variable <code>CUDA_HOME</code> is set by the <code>cuda</code> module.</p>"},{"location":"hpc/software/libraries/cuda/#additional-references","title":"Additional References","text":"<ul> <li>CUDA Toolkit Information </li> <li>CUDA Toolkit 12.2.1 Documentation </li> <li>CUDA Toolkit 11.8.0 Documentation </li> </ul>"},{"location":"hpc/software/libraries/fftw/","title":"FFTW","text":""},{"location":"hpc/software/libraries/fftw/#loading-fftw","title":"Loading FFTW","text":"<p>FFTW on Lawrencium can be loaded after loading a MPI library. For example, to load FFTW installed using the default gcc compiler and the default Open MPI on Lawrencium:</p> <p><pre><code>[user@n0000 ~]$ module load gcc openmpi\n[user@n0000 ~]$ module avail fftw\n\n--------- /global/software/rocky-8.x86_64/modfiles/openmpi/4.1.6-4xq5u5r/gcc/11.4.0 --------\n   fftw/3.3.10\n</code></pre> <pre><code>[user@n0000 ~]$ module load fftw/3.3.10\n</code></pre></p>"},{"location":"hpc/software/libraries/fftw/#compiling-programs-using-fftw-library","title":"Compiling programs using FFTW library","text":"<p>To compile using the loaded <code>fftw3</code> library, we need the appropriate <code>CFLAGS</code> and <code>LDFLAGS</code> during compilation and linking. These can be obtained in Lawrencium using </p> <pre><code>[user@n0000 ~]$ pkg-config --cflags --libs fftw3\n-I/global/software/rocky-8.x86_64/gcc/linux-rocky8-x86_64/gcc-11.4.0/fftw-3.3.10-cf4npbktueip6tnwqf2qstog7on4pyfk/include -L/global/software/rocky-8.x86_64/gcc/linux-rocky8-x86_64/gcc-11.4.0/fftw-3.3.10-cf4npbktueip6tnwqf2qstog7on4pyfk/lib -lfftw3 \n</code></pre> <p>Note that the result above does not include linker flags for MPI FFTW routines. To compile program using <code>MPI FFTW</code>, in addition to <code>-lfftw3</code> we also need <code>-lfftw3_mpi</code> and <code>-lm</code> (see here ).</p> <p>Therefore, to compile using MPI FFTW library:</p> <pre><code>mpicc -o output $(pkg-config --cflags --libs fftw3) -lfftw_mpi -lm example_mpi_fftw.c\n</code></pre> <p>Compiling using <code>rpath</code></p> <p>To compile using <code>rpath</code>, you need to add the following:</p> <pre><code>-Wl,-rpath,$(pkg-config --variable=libdir fftw3)\n</code></pre> <p>Compiling with <code>rpath</code> adds the <code>libdir</code> to the runtime library search path in the executable file.</p>"},{"location":"hpc/software/libraries/hdf5/","title":"HDF5","text":""},{"location":"hpc/software/libraries/hdf5/#loading-hdf5","title":"Loading HDF5","text":"<p>HDF5 on Lawrencium can be loaded after loading a MPI library. For example, to load HDF5 installed under the default gcc compiler and the default Open MPI on Lawrencium:</p> <p><pre><code>[user@n0000 ~]$ module load gcc openmpi\n[user@n0000 ~]$ module avail hdf5\n\n------------- /global/software/rocky-8.x86_64/modfiles/openmpi/4.1.6-4xq5u5r/gcc/11.4.0 --------------\n   hdf5/1.14.3\n</code></pre> <pre><code>[user@n0000 ~]$ module load hdf5\n</code></pre></p>"},{"location":"hpc/software/libraries/hdf5/#compiling-programs-using-hdf5-library","title":"Compiling programs using HDF5 library","text":"<p>Let's look at an example of compiling a simple C example <code>ph5_file_create.c</code> from hdf5-examples . The example creates a HDF5 file named <code>SDS_row.h5</code>.</p> <p>To compile using the loaded <code>HDF5</code> library, we need the appropriate <code>CFLAGS</code> and <code>LDFLAGS</code> during compilation and linking. These can be obtained in Lawrencium using </p> <pre><code>[user@n0000 ~]$ pkg-config --cflags --libs hdf5\n-I/global/software/rocky-8.x86_64/gcc/linux-rocky8-x86_64/gcc-11.4.0/hdf5-1.14.3-6763puu3e5vxq4vmbaosgiv4yhzjb46s/include -L/global/software/rocky-8.x86_64/gcc/linux-rocky8-x86_64/gcc-11.4.0/hdf5-1.14.3-6763puu3e5vxq4vmbaosgiv4yhzjb46s/lib -lhdf5 \n</code></pre> <p>To include these directly in the compilation process, we can do the following:</p> <pre><code>mpicc -o ph5_file_create $(pkg-config --cflags --libs hdf5) ph5_file_create.c\n</code></pre>"},{"location":"hpc/software/libraries/mkl/","title":"Intel MKL Library","text":"<p>Intel MKL library is available under both <code>gcc</code> and <code>intel-oneapi-compilers</code> on Lawrencium. Intel MKL library can be loaded after loading a compiler/mpi combination. For example:</p> <pre><code>[user@n0000 ~]$ module load gcc openmpi\n[user@n0000 ~]$ module load intel-oneapi-mkl\n[user@n0000 ~]$ module list\n\nCurrently Loaded Modules:\n  1) gcc/11.4.0   2) ucx/1.14.1   3) openmpi/4.1.6   \n  4) intel-oneapi-tbb/2021.10.0   5) intel-oneapi-mkl/2023.2.0\n</code></pre> <p>Similarly, we can load the MKL library with the intel oneapi compilers and mpi as:</p> <pre><code>[user@n0000 ~]$ module load intel-oneapi-compilers\n[user@n0000 ~]$ module load intel-oneapi-mpi\n[user@n0000 ~]$ module load intel-oneapi-mkl\n[user@n0000 ~]$ module list\n\nCurrently Loaded Modules:\n  1) intel-oneapi-compilers/2023.1.0   3) intel-oneapi-tbb/2021.10.0\n  2) intel-oneapi-mpi/2021.10.0        4) intel-oneapi-mkl/2023.2.0\n</code></pre> <p>MKL Link Line Advisor</p> <p>Use the Intel\u00ae oneAPI Math Kernel Library (oneMKL) Link Line Advisor tool   to obtain the appropriate compiler and linker options depending on your use case.</p>"},{"location":"hpc/software/libraries/netcdf/","title":"NetCDF","text":""},{"location":"hpc/software/libraries/netcdf/#loading-netcdf","title":"Loading netCDF","text":"<p>NetCDF on Lawrencium can be loaded after loading a MPI library. For example, to load netCDF installed using the default gcc compiler and the default Open MPI on Lawrencium:</p> <pre><code>[user@n0000 ~]$ module load gcc openmpi\n[user@n0000 ~]$ module avail netcdf\n\n------------- /global/software/rocky-8.x86_64/modfiles/openmpi/4.1.6-4xq5u5r/gcc/11.4.0 --------------\n   netcdf-c/4.9.2    netcdf-fortran/4.6.1\n</code></pre> <p>As you can see on the output of <code>module avail netcdf</code>, a C version of the library <code>netcdf-c</code> and a fortran version of the library <code>netcdf-fortran</code> are available.</p>"},{"location":"hpc/software/libraries/netcdf/#compiling-programs-using-netcdf-library","title":"Compiling programs using netCDF library","text":"<p>Let's look at an example of compiling a simple fortran example <code>simple_xy_rd.f90</code> from Example netCDF programs  . The example creates a netcdf file with a two-dimensional array of sample data.</p> <p>To compile using the <code>netcdf-fortran</code> library, we need the appropriate <code>CFLAGS</code> and <code>LDFLAGS</code> during compilation and linking. These can be obtained in Lawrencium using </p> <pre><code>[user@n0000 ~]$ pkg-config --cflags netcdf-fortran\n-I/global/software/rocky-8.x86_64/gcc/linux-rocky8-x86_64/gcc-11.4.0/netcdf-fortran-4.6.1-fjshq66ynuoqqbtns2n3pwerlpymqjkg/include -I/global/software/rocky-8.x86_64/gcc/linux-rocky8-x86_64/gcc-11.4.0/netcdf-c-4.9.2-heo4zhdmupk4ru7x6aujkoptuceeilh2/include \n\n[user@n0000 ~]$ pkg-config --libs netcdf-fortran\n-L/global/software/rocky-8.x86_64/gcc/linux-rocky8-x86_64/gcc-11.4.0/netcdf-fortran-4.6.1-fjshq66ynuoqqbtns2n3pwerlpymqjkg/lib -lnetcdff\n</code></pre> <p>To include these directly in the compilation process, we can do the following:</p> <pre><code>gfortran -o simple_xy_rd $(pkg-config --cflags --libs netcdf-fortran) simple_xy_rd.f90\n</code></pre> <p>Before running the binary <code>simple_xy_rd</code>, you have to add the <code>netcdf-fortran</code> library path to the <code>LD_LIBRARY_PATH</code> environment variable.</p> <pre><code>export LD_LIBRARY_PATH=/global/software/rocky-8.x86_64/gcc/linux-rocky8-x86_64/gcc-11.4.0/netcdf-fortran-4.6.1-fjshq66ynuoqqbtns2n3pwerlpymqjkg/lib:$LD_LIBRARY_PATH\n</code></pre>"},{"location":"hpc/software/ml/alphafold3/","title":"AlphaFold 3 on Lawrencium","text":"<p>AlphaFold 3  is a new AI model developed by Google DeepMind  and Isomorphic Labs  for generating 3D predictions of biological systems. The software package and the public database is now available ont the Lawrencium cluster.</p>"},{"location":"hpc/software/ml/alphafold3/#genetic-databases","title":"Genetic Databases","text":"<p>The genetic database required for AlphaFold 3 is saved under the shared directory <code>/clusterfs/collections/Alphafold3/public-db</code> on the cluster.</p>"},{"location":"hpc/software/ml/alphafold3/#model-parameters","title":"Model Parameters","text":"<p>The model parameters are the result of training the AlphaFold model and required for inference pipeline of AlphaFold 3. The model parameters are distributed separately from the source code by Google DeepMind and subject to terms Model Parameters Terms of Use .</p> <p>Lawrencium users interested in using AlphaFold 3 are required to abide to above terms. Users can request a personal copy of the model parameters directly from Google DeepMind by filling out this form. If you have any questions about fields of the form then you may send us an inquiry at hpcshelp@lbl.gov. Once you get response and directions from Google DeepMind on obtaining model parameters you may save the parameters file in your home directory or project directory (if you are sharing with your group members) inside directory <code>model_param</code>. The parameters file is a single file approximately 1GB in size.</p>"},{"location":"hpc/software/ml/alphafold3/#loading-alphafold-3-module","title":"Loading AlphaFold 3 module","text":"<pre><code>module load ml/alphafold3\n</code></pre> <p>The <code>ml/alphafold3</code> module defines various environment variables such as <code>ALPHAFOLD_DIR</code> and <code>DB_DIR</code> that can be used to run a job as shown below. Users will have to setup environment variable for <code>MODEL_PARAMETERS_DIR</code> before running the script or it can be setup directly in the job submission script as shown below.</p>"},{"location":"hpc/software/ml/alphafold3/#running","title":"Running","text":"<p>Note</p> <ul> <li>Use <code>python /app/alphafold/run_alphafold.py</code> when using the <code>alphafold3.sif</code> image from the <code>ml/alphafold3</code> module. This is different from the official instructions on the <code>alphafold3</code> github page. See the sample slurm script below.</li> </ul> <p>Below is a sample script to run <code>alphafold3</code> after loading <code>ml/alphafold3</code> module. It assumes the presence of <code>fold_input.json</code> in <code>$HOME/af_input</code> and saves output to <code>$HOME/af_output</code>. Please pay close attention to different options, path and variables and make changes as necessary.</p> <pre><code>#!/bin/bash\n#SBATCH --account=&lt;account&gt;\n#SBATCH --partition=es1\n#SBATCH --gres=gpu:H100:1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=14\n#SBATCH --nodes=1\n#SBATCH --qos=es_normal\n#SBATCH --time=1:30:00\n\nmodule load ml/alphafold3\n\n# export MODEL_PARAMETERS_DIR variable according to where they are saved\n#If model parameters are saved in your home directory\nexport MODEL_PARAMETERS_DIR=/global/home/users/$USER/model_param\n\n#If model parameters are saved in your group directory\nexport MODEL_PARAMETERS_DIR=/global/home/groups/&lt;project_name&gt;/model_param\n\napptainer exec --nv --bind $HOME/af_input:/root/af_input \\\n                    --bind $HOME/af_output:/root/af_output \\\n                    --bind $MODEL_PARAMETERS_DIR:/root/models \\\n                    --bind $DB_DIR:/root/public_databases \\\n                    $ALPHAFOLD_DIR/alphafold3.sif \\\n                    python /app/alphafold/run_alphafold.py \\\n                    --json_path=/root/af_input/fold_input.json \\\n                    --model_dir=/root/models \\\n                    --db_dir=/root/public_databases \\\n                    --output_dir=/root/af_output\n</code></pre>"},{"location":"hpc/software/ml/pytorch/","title":"PyTorch","text":""},{"location":"hpc/software/ml/pytorch/#loading-pytorch","title":"Loading PyTorch","text":"<pre><code>module load ml/pytorch\n</code></pre> <p>PyTorch versions</p> <p>Use <code>module spider pytorch</code> to get information on the versions of pytorch available as modules.</p> <p><code>module load ml/pytorch</code> will additionally load other dependent modules such as <code>cuda</code>.</p> <p>If you use jupyter server on lrc-openondemand, pytorch kernels <code>torch 2.0.1</code> ard <code>torch 2.3.1</code> are available.  </p>"},{"location":"hpc/software/ml/pytorch/#multi-gpu-jobs","title":"Multi-GPU jobs","text":"<p>A sample for a multi-GPU PyTorch code can be found on the Distributed PyTorch tutorial  examples on github. The SLURM script provided in the pytorch examples folder can be adapted to run on our cluster. The SLURM script provided below runs the <code>multinode.py</code> pytorch script on four A40 GPU cards distributed over two nodes:</p> <pre><code>#SBATCH --job-name=ddp_on_A40\n#SBATCH --partition=es1\n#SBATCH --nodes=2\n#SBATCH --ntasks-per-node=1\n#SBATCH --cpus-per-task=4\n#SBATCH --account=&lt;ACCOUNT_NAME&gt;\n#SBATCH --time=01:00:00\n#SBATCH --qos=es_normal\n#SBATCH --gres=gpu:A40:2\n\nmodule load ml/pytorch\n\nallocated_nodes=$(scontrol show hostname $SLURM_JOB_NODELIST)\nnodes=${allocated_nodes//$'\\n'/ }\nnodes_array=($nodes)\nhead_node=${nodes_array[0]}\n\necho Head Node: $head_node\necho Node List: $nodes\n\nsrun torchrun --nnodes 2 \\\n              --nproc_per_node 2 \\\n              --rdzv_id $RANDOM \\\n              --rdzv_backend c10d \\\n              --rdzv_endpoint $head_node:29500 \\\n              $cwd/multinode.py 500 10\n</code></pre>"},{"location":"hpc/software/ml/tensorflow/","title":"Tensorflow","text":""},{"location":"hpc/software/ml/tensorflow/#loading-tensorflow","title":"Loading Tensorflow","text":"<pre><code>module load ml/tensorflow\n</code></pre> <p>Tensorflow versions</p> <p>Use <code>module spider tensorflow</code> to get information on the versions of pytorch available as modules.</p> <p><code>module load ml/tensorflow</code> will load other dependent modules such as <code>cuda</code>.</p> <p>If you use jupyter server on lrc-openondemand, tensorflow kernels <code>tf 2.15.0</code> ard <code>tf 2.14.0</code> are available.  </p>"},{"location":"hpc/software/ml/tensorflow/#example-slurm-script","title":"Example SLURM script","text":"<p>The follow SLURM script shows how to run a tensorflow script on 1 H100 GPU card. <pre><code>#!/bin/bash\n#SBATCH --job-name=\"TensorFlowCIFAR10\"\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=14\n#SBATCH --output=tf_job.out\n#SBATCH --error=tf_job.err\n#SBATCH --time=0:40:0\n#SBATCH --partition=es1\n#SBATCH --account=&lt;ACCOUNT_NAME&gt;\n#SBATCH --qos=es_normal\n#SBATCH --gres=gpu:H100:1\n\nmodule load ml/tensorflow\nsrun python cifar10.py\n</code></pre></p>"},{"location":"hpc/software/mpi/intelmpi/","title":"Intel MPI","text":""},{"location":"hpc/software/mpi/intelmpi/#loading-intel-mpi","title":"Loading Intel MPI","text":"<pre><code>[user@n0000 ~]$ module load intel-oneapi-compilers\n[user@n0000 ~]$ module load intel-oneapi-mpi\n[user@n0000 ~]$ module list\n\nCurrently Loaded Modules:\n  1) intel-oneapi-compilers/2023.1.0   2) intel-oneapi-mpi/2021.10.0\n</code></pre>"},{"location":"hpc/software/mpi/intelmpi/#compiling-mpi-applications-with-intel-mpi","title":"Compiling MPI applications with Intel MPI","text":"<p>Open MPI compiler wrappers <code>mpiicx</code>, <code>mpiicpx</code>, <code>mpiifx</code> can be used to compile MPI applications. For hello world C/C++/Fortran examples:</p> <p>Examples</p> CC++Fortran <pre><code>mpiicx -o helloc hello_world.c\n</code></pre> <p><code>mpiicx</code> is the MPI wrapper to the Intel(R) C/C++ compiler and should be used to compile and link C programs</p> <pre><code>mpiicpx -o hellocxx hello_world.cpp\n</code></pre> <p><code>mpiicpx</code> is the MPI wrapper to the Intel(R) oneAPI DPC++/C++ Compiler and should be used to compile and link C++ programs</p> <pre><code>mpiifx -o hellofortran hello_world.f90\n</code></pre> <p><code>mpiifx</code> is the MPI wrapper to the Intel(R) oneAPI Fortran Compiler <code>ifx</code>.</p> <p>The <code>intel-oneapi-mpi</code> package also comes with MPI wrapper to the Intel Classic Compilers: <code>mpiicc</code>, <code>mpiicpc</code> and <code>mpiifort</code>.</p>"},{"location":"hpc/software/mpi/intelmpi/#running-mpi-applications-using-intel-mpi","title":"Running MPI applications using Intel MPI","text":"<p>Intel MPI applications can be launched using:</p> <ul> <li><code>mpirun</code> e.g.: <code>mpirun -np 2 ./helloc</code></li> <li><code>srun</code>: To launch an Intel MPI application using <code>srun</code>, please set the <code>I_MPI_PMI_LIBRARY</code> environment variable and pass <code>mpi=pmi2</code> argument as follows in your slurm script.       <pre><code>export I_MPI_PMI_LIBRARY=/usr/lib64/libpmi2.so\nsrun --mpi=pmi2 mpi_application\n</code></pre></li> </ul>"},{"location":"hpc/software/mpi/openmpi/","title":"Open MPI","text":""},{"location":"hpc/software/mpi/openmpi/#loading-open-mpi","title":"Loading Open MPI","text":"<p>Open MPI is installed for various compilers in the software module farm. A compiler must be loaded before you can load the corresponding <code>openmpi</code>. For example, if you load the default <code>gcc</code> through <code>module load gcc</code>, then you can see which openmpi modules are available under the <code>gcc</code> module via <code>module avail openmpi</code>:</p> <pre><code>[user@n0000 ~]$ module load gcc\n[user@n0000 ~]$ module list\n\nCurrently Loaded Modules:\n  1) gcc/11.4.0\n\n[user@n0000 ~]$ module avail openmpi\n\n-------------- /global/software/rocky-8.x86_64/modfiles/gcc/11.4.0 -----------\n   openmpi/4.1.3    openmpi/4.1.6 (D)\n</code></pre> <p>After this, you can load the default <code>openmpi/4.1.6</code> through <code>module load openmpi</code> or by specifying the version <code>module load openmpi/4.1.6</code>. If you want to load the non-default <code>openmpi/4.1.3</code> module, then you must specify the version: <code>module load openmpi/4.1.3</code>:</p> <pre><code>[user@n0000 ~]$ module load openmpi\n[user@n0000 ~]$ module list\n\nCurrently Loaded Modules:\n  1) gcc/11.4.0   2) ucx/1.14.1   3) openmpi/4.1.6\n</code></pre>"},{"location":"hpc/software/mpi/openmpi/#compiling-mpi-applications-with-open-mpi","title":"Compiling MPI applications with Open MPI","text":"<p>Open MPI compiler wrappers <code>mpicc</code>, <code>mpicxx</code>, <code>mpifort</code> can be used to compile MPI applications. For hello world C/C++/Fortran examples:</p> <p>Examples</p> CC++Fortran <pre><code>mpicc -o helloc hello_world.c\n</code></pre> <p><code>mpicc</code> is the MPI wrapper to the gcc C compiler.</p> <pre><code>mpicxx -o hellocxx hello_world.cpp\n</code></pre> <p><code>mpicxx</code> is the MPI wrapper to the gcc C++ compiler.</p> <pre><code>mpifort -o hellofortran hello_world.f90\n</code></pre> <p><code>mpifort</code> is the MPI wrapper to the gfortran compiler.</p> <p>The <code>gcc/openmpi</code> compiled binaries can be launched directly through <code>srun</code> inside of a slrum job script.</p>"},{"location":"hpc/systems/einsteinium/","title":"Einsteinium GPU Cluster","text":"<p>Einsteinium is an institutional GPU cluster that was deployed to meet the growing computational demand for researchers doing machine learning and deep learning. The system is named after the chemical element with symbol <code>Es</code> and atomic number 99 which was discovered at Lawrence Berkeley National Laboratory in 1952 and in honor of Albert Einstein who developed the theory of relativity.</p>"},{"location":"hpc/systems/einsteinium/#es1-partition","title":"<code>es1</code> Partition","text":"<p><code>es1</code> is a partition consisting of multiple GPU node types to address the different research needs. These include:</p> Accelerator Nodes GPUs per Node/GPU Memory CPU Processor CPU Cores CPU RAM Infiniband NVIDIA H100 4 8x 80 GB Intel Xeon Platinum 8480+ 112 1 TB NDR NVIDIA A100 1 4x 80 GB AMD EPYC 7713 64 512 GB HDR NVIDIA A40 30 4x 48 GB AMD EPYC 7742 64 512 GB FDR NVIDIA GRTX8000 1 4x 48 GB AMD EPYC 7713 64 512 GB HDR NVIDIA V100 15 2x 32 GB Intel Xeon E5-2623 8 64GB or 192GB FDR <p>H100 and CBORG</p> <p>Currently, we have five NVIDIA H100 nodes in our datacenter, four of which are available to users through SLURM on <code>es1</code> partition. One H100 node (8 GPUs) is used for LLM inference by CBORG .</p>"},{"location":"hpc/systems/einsteinium/#how-to-specify-desired-gpu-cards","title":"How to specify desired GPU card(s)","text":"<p>Due to hardware configuation, special attention is needed to ensure the ratio of CPU-core# to GPU#</p> <p>Examples:</p> <ul> <li>Request one V100 card: <code>--cpus-per-task=4 --gres=gpu:V100:1 --ntasks=1</code> </li> <li>Request two A40 cards: <code>--cpus-per-task=16 --gres=gpu:A40:2 --ntasks=2</code></li> <li>Request three H100 cards: <code>--cpus-per-task=14 --gres=gpu:H100:3 --ntasks=3</code> </li> <li>Request one A100 cards: <code>--cpus-per-task=16 --gres=gpu:A100:1 --ntasks=1</code> </li> <li>Request four GRTX8000 cards: <code>--cpus-per-task=16 --gres==gpu:GRTX8000:4 --ntasks=4</code> </li> </ul> <p>Example slurm script on <code>es1</code></p> <p>Here is an example slurm script that requests one NVIDIA A40 GPU card.</p> Single GPU on a A40 node4 GPUs on a H100 node <pre><code>#!/bin/bash\n#SBATCH --job-name=test\n#SBATCH --account=account_name\n#SBATCH --partition=es1\n#SBATCH --qos=es_normal\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=16\n#SBATCH --gres=gpu:A40:1\n#SBATCH --time=1:00:00\n\nmodule load ml/pytorch\npython train.py\n</code></pre> <pre><code>#!/bin/bash\n#SBATCH --job-name=test\n#SBATCH --account=account_name\n#SBATCH --partition=es1\n#SBATCH --qos=es_normal\n#SBATCH --nodes=1\n#SBATCH --ntasks=4\n#SBATCH --cpus-per-task=14\n#SBATCH --gres=gpu:H100:4\n#SBATCH --time=1:00:00\n\nmodule load ml/pytorch\npython train.py\n</code></pre>"},{"location":"hpc/systems/einsteinium/#es0-partition","title":"<code>es0</code> Partition","text":"<p><code>es0</code> is a partition with NVIDIA 2080 TI GPUs that do not incur Service Unit (SU) charges.</p> Accelerator Nodes GPUs per Node/GPU Memory CPU Processor CPU Cores CPU RAM Infiniband NVIDIA 2080TI 12 4x 11 GB Intel Xeon Silver 4212 8 96GB FDR <p>Example slurm script on <code>es0</code></p> Single GPUFour GPUs on a node <pre><code>#!/bin/bash\n#SBATCH --job-name=testes0\n#SBATCH --account=account_name\n#SBATCH --partition=es0\n#SBATCH --qos=es_normal\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --cpus-per-task=2\n#SBATCH --gres=gpu:1\n#SBATCH --time=1:00:00\n\nmodule load ml/pytorch\npython train.py\n</code></pre> <pre><code>#!/bin/bash\n#SBATCH --job-name=testes0\n#SBATCH --account=account_name\n#SBATCH --partition=es0\n#SBATCH --qos=es_normal\n#SBATCH --nodes=1\n#SBATCH --ntasks=4\n#SBATCH --cpus-per-task=2\n#SBATCH --gres=gpu:4\n#SBATCH --time=1:00:00\n\nmodule load ml/pytorch\npython train.py\n</code></pre>"},{"location":"hpc/systems/lawrencium/","title":"CPU Cluster","text":""},{"location":"hpc/systems/lawrencium/#lawrencium","title":"Lawrencium","text":"<p>About Lawrencium Cluster</p> <p>Lawrencium is a general purpose cluster that is suitable for running a wide variety of scientific applications. The system is named after the chemical element 103 which was discovered at Lawrence Berkeley National Laboratory in 1958 and in honor of Ernest Orlando Lawrence, the inventor of the cyclotron. </p> <p>The original Lawrencium system was built as a 200-node cluster and debuted as #500 on the Top500 supercomputing list in Nov 2008.</p> <p>Lawrencium consists of multiple generations of compute nodes with the <code>lr8</code> partition being the most recent addition and the <code>lr4</code> partition the oldest still in production. In addition, there is a <code>lr_bigmem</code> partition with 1.5TB memory per node, and <code>cm1, cm2, cf1</code> partitions (details in the table below).</p> Partition Nodes CPU Cores Memory Infiniband lr8 20 AMD EPYC 9534 128 768GB HDR lr7 132 Intel Xeon Gold 6330 56 256GB or 512GB HDR lr6 88 Intel Xeon Gold 6130 32 96GB or 128GB FDR 156 Intel Xeon Gold 5218 32 96GB FDR Intel Xeon Gold 6230 40 128GB FDR lr5 192 Intel Xeon E5-2680v4 28 64GB FDR Intel Xeon E5-2640v4 20 128GB QDR lr4 148 Intel Xeon E5-2670v3 24 64GB FDR lr_bigmem 2 Intel Xeon Gold 5218 32 1.5TB EDR cm1 14 AMD EPYC 7401 48 256GB FDR cm2 3 AMD EPYC 7454 64 256GB EDR cf1 72 Intel Xeon Phi 7210 256 192GB FDR <p>LRC Jobscript Generator</p> <p>You can use the LRC Jobscript Generator page to generate sample slurm job submission scripts targeting these different systems.</p>"},{"location":"hpc/systems/supported/alsacc/","title":"ALSACC - Advanced Light Source","text":"<p>The ALSACC cluster is part of the LBNL Supercluster and shares the same Supercluster infrastructure. This includes the system management software, software module farm, scheduler, storage and backend network management.</p>"},{"location":"hpc/systems/supported/alsacc/#login-and-data-transfer","title":"Login and Data Transfer","text":"<p>ALSACC uses One Time Password (OTP) for login authentication for all the services provided below. Please also refer to the Data Transfer page for additional information.</p> <ul> <li>Login server: <code>lrc-login.lbl.gov</code></li> <li>DATA transfer server: <code>lrc-xfer.lbl.gov</code></li> <li>Globus Online endpoint: <code>lbnl#lrc</code></li> </ul>"},{"location":"hpc/systems/supported/alsacc/#hardware-configuration","title":"Hardware Configuration","text":"<p>ALSACC cluster has a mixture of different CPU architectures and memory configurations so please be aware of them and choose them wisely along with the scheduler configurations.</p> Partition Nodes Node List CPU Cores Memory Infiniband alsacc 64 n00[00-27].alsacc0 Intel Xeon X5650 12 24GB QDR n00[28-43].alsacc0 Intel Xeon E5-2670 16 64GB FDR n00[44-55].alsacc0 Intel Xeon E5-2670v2 20 64GB FDR n00[56-63].alsacc0 Intel Xeon E5-2670v3 24 64GB FDR"},{"location":"hpc/systems/supported/alsacc/#storage-and-backup","title":"Storage and Backup","text":"<p>ALSACC cluster users are entitled to access the following storage systems so please get familiar with them.</p> Name Location Quota Backup Allocation Description HOME <code>/global/home/users/$USER</code> 12GB Yes Per User HOME directory for permanent data storage GROUP-SW <code>/global/home/groups-sw/$GROUP</code> 200GB Yes Per Group GROUP directory for software and data sharing with backup GROUP <code>/global/home/groups/$GROUP</code> 400GB No Per Group GROUP directory for data sharing without backup SCRATCH <code>/global/scratch/$USER</code> none No Per User SCRATCH directory with Lustre high performance parallel file system CLUSTERFS <code>/clusterfs/alsacc/$USER</code> none No Per User Private storage <p>Note</p> <p>HOME, GROUP, and GROUP-SW directories are located on a highly reliable enterprise level BlueArc storage device. Since this appliance also provides storage for many other mission critical file systems, and it is not designed for high performance applications, running large I/O dependent jobs on these file systems could greatly degrade the performance of all the file systems that are hosted on this device and affect hundreds of users, thus this behavior is explicitly prohibited. HPCS reserves the right to kill these jobs without notification once discovered. Jobs that have I/O requirement should use the SCRATCH file system which is designed specifically for that purpose.</p>"},{"location":"hpc/systems/supported/alsacc/#scheduler-configuration","title":"Scheduler Configuration","text":"<p>ALSACC cluster uses SLURM as the scheduler to manage jobs on the cluster. To use the ALSACC resource, the partition <code>alsacc</code> must be used (<code>--partition=alsacc</code>) along with account <code>alsacc</code> (<code>--account=alsacc</code>). Currently there is no special limitation introduced to the <code>alsacc</code> partition thus no QoS configuration is required to use the ALSACC resources (a default QoS will be applied automatically). A standard fair-share policy with a decay half life value of 14 days is enforced.</p> <p>The job allocation on ALSACC is shared i.e. a node can be shared between multiple jobs. The different QoS arguments and their limits are shown below:</p> Node List Node Features n00[00-27].alsacc0 alsacc, alsacc_c12 n00[28-43].alsacc0 alsacc, alsacc_c16 n00[44-55].alsacc0 alsacc, alsacc_c20 n00[56-63].alsacc0 alsacc, alsacc_c24"},{"location":"hpc/systems/supported/alsacc/#software-configuration","title":"Software Configuration","text":"<p>ALSACC uses Software Module Farm to manage the cluster-wide software installation.</p>"},{"location":"hpc/systems/supported/alsacc/#cluster-status","title":"Cluster Status","text":"<p>Please visit here  for the live status of ALSACC cluster.</p>"},{"location":"hpc/systems/supported/catamount/","title":"CATAMOUNT - Material Sciences Division","text":"<p>The CATAMOUNT cluster is part of the LBNL Supercluster and shares the same Supercluster infrastructure. This includes the system management software, software module farm, scheduler, storage, and backend network management.</p>"},{"location":"hpc/systems/supported/catamount/#login-and-data-transfer","title":"Login and Data Transfer","text":"<p>CATAMOUNT uses One Time Password (OTP) for login authentication for all the services provided below. Please also refer to the Data Transfer page for additional information.</p> <ul> <li>Login server: <code>lrc-login.lbl.gov</code></li> <li>DATA transfer server: <code>lrc-xfer.lbl.gov</code></li> <li>Globus Online endpoint: <code>lbnl#lrc</code></li> </ul>"},{"location":"hpc/systems/supported/catamount/#hardware-configuration","title":"Hardware Configuration","text":"<p>Each compute node has dual-socket octa-core Intel Xeon E5-2670 (Sandy Bridge) @ 2.60 GHz processors (16 cores in total), 64 GB of physical memory.  Compute nodes are connected with each other through multiple high performance Mellanox 56 Gbps FDR Infiniband edge switches then back to a backbone Mellanox SX6518 director switch.</p> Partition Nodes Node List CPU Cores Memory catamount 116 n0[000-115].catamount0 Intel Xeon E5-2670 16 64GB"},{"location":"hpc/systems/supported/catamount/#storage-and-backup","title":"Storage and Backup","text":"<p>CATAMOUNT cluster users are entitled to access the following storage systems so please get familiar with them.</p> Name Location Quota Backup Allocation Description HOME <code>/global/home/users/$USER</code> 12GB Yes Per User HOME directory for permanent data storage GROUP-SW <code>/global/home/groups-sw/$GROUP</code> 200GB Yes Per Group GROUP directory for software and data sharing with backup GROUP <code>/global/home/groups/$GROUP</code> 400GB No Per Group GROUP directory for data sharing without backup SCRATCH <code>/global/scratch/$USER</code> none No Per User SCRATCH directory with Lustre high performance parallel file system CLUSTERFS <code>/clusterfs/catamount/$USER</code> none No Per User Private storage <p>Note</p> <p>HOME, GROUP, and GROUP-SW directories are located on a highly reliable enterprise level BlueArc storage device. Since this appliance also provides storage for many other mission critical file systems, and it is not designed for high performance applications, running large I/O dependent jobs on these file systems could greatly degrade the performance of all the file systems that are hosted on this device and affect hundreds of users, thus this behavior is explicitly prohibited. HPCS reserves the right to kill these jobs without notification once discovered. Jobs that have I/O requirement should use the SCRATCH file system which is designed specifically for that purpose.</p>"},{"location":"hpc/systems/supported/catamount/#scheduler-configuration","title":"Scheduler Configuration","text":"<p>CATAMOUNT cluster uses SLURM as the scheduler to manage jobs on the cluster. To use the CATAMOUNT resource, the partition <code>catamount</code> must be used (<code>--partition=catamount</code>) along with account <code>catamount</code> (<code>--account=catamount</code>). One of the QoSs from the following table should be used as well (e.g., <code>--qos=cm_short</code>). A standard fair-share policy with a decay half life value of 14 days (2 weeks) is enforced.</p> <p>The job allocation on CATAMOUNT is exclusive i.e. a node is not shared between two jobs. The different QoS arguments and their limits are shown below:</p> QOS QOS Limit cm_short 4 nodes max per job; 24:00:00 wallclock limit cm_medium 16 nodes max per job; 72:00:00 wallclock limit cm_long 32 nodes max per user cm_debug 2 nodes max per job; 8 nodes in total; 00:30:00 wallclock limit"},{"location":"hpc/systems/supported/catamount/#software-configuration","title":"Software Configuration","text":"<p>CATAMOUNT uses Software Module Farm to manage the cluster-wide software installation.</p>"},{"location":"hpc/systems/supported/catscan/","title":"Catscan","text":"<p>The Catscan cluster is an XCAT stand alone cluster.</p> <p>Login node: catscan.lbl.gov</p> <p>Sports a 269Tb zfs filesystem for data and computational scratch space on /pool0 (see below)</p> <p>Compute nodes: n0000, n0001, n0002, n0003</p>"},{"location":"hpc/systems/supported/catscan/#cluster-configuration","title":"Cluster Configuration","text":"Node Access Storage Filesystems Description of Use CPU CORES MEMORY GPU catscan.lbl.gov ssh with either LDAP credentials or password provided by administrator /home: local drive, 211G   /pool0 ZFS filesystem, 269T /clusterfs/bebb/users   /clusterfs/bebb/group-sw Login node Intel(R) Xeon(R) Gold 6126 48 (HT Enabled) 196 GB N/A n000[0-1] ssh from catscan.lbl.gov with cluster key As above via nfs As above via nfs Compute node Intel(R) Xeon(R) Gold 6126 48 (HT Enabled) 196 GB 4x NVIDIA GeForce GTX 1080 Ti n000[2-3] ssh from catscan.lbl.gov with cluster key As above via nfs As above via nfs Compute node 24 188 GB 2x NVIDIA RTX A4500 <p>Operating System: CentOS Linux release 7.9.2009 (Core)</p> <p>Nvidia driver &amp; NVRM version: NVIDIA UNIX x86_64 Kernel Module  510.73.05  Sat May  7 05:30:26 UTC 2022</p> <p>Each compute node has 4 GPUS:</p> <pre><code>CUDA Driver Version / Runtime Version          10.1 / 10.0 \nCUDA Capability Major/Minor version number:    6.1 \nTotal amount of global memory:                 11178 MBytes (11721506816 bytes) \n(28) Multiprocessors, (128) CUDA Cores/MP:     3584 CUDA Cores \nGPU Max Clock rate:                            1582 MHz (1.58 GHz) \nMemory Clock rate:                             5505 Mhz \nMemory Bus Width:                              352-bit L2 \nCache Size:                                 2883584 bytes \nMaximum Texture Dimension Size (x,y,z)         1D=(131072), 2D=(131072, 65536), 3D=(16384, 16384, 16384) \nMaximum Layered 1D Texture Size, (num) layers  1D=(32768), 2048 layers \nMaximum Layered 2D Texture Size, (num) layers  2D=(32768, 32768), 2048 layers \nTotal amount of constant memory:               65536 bytes \nTotal amount of shared memory per block:       49152 bytes \nTotal number of registers available per block: 65536 \nWarp size:                                     32 \nMaximum number of threads per multiprocessor:  2048 \nMaximum number of threads per block:           1024 \nMax dimension size of a thread block (x,y,z): (1024, 1024, 64) \nMax dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535) \nMaximum memory pitch:                          2147483647 bytes \nTexture alignment:                             512 bytes \nConcurrent copy and kernel execution:          Yes with 2 copy engine(s) \nRun time limit on kernels:                     No \nIntegrated GPU sharing Host Memory:            No \nSupport host page-locked memory mapping:       Yes \nAlignment requirement for Surfaces:            Yes \nDevice has ECC support:                        Disabled \nDevice supports Unified Addressing (UVA):      Yes \nDevice supports Compute Preemption:            Yes \nSupports Cooperative Kernel Launch:            Yes \nSupports MultiDevice Co-op Kernel Launch:      Yes \nDevice PCI Domain ID / Bus ID / location ID:   0 / 94 / 0 \nCompute Mode: &lt; Default (multiple host threads can use ::cudaSetDevice() with device simultaneously) &gt;\n</code></pre>"},{"location":"hpc/systems/supported/catscan/#queue-configuration","title":"Queue Configuration","text":"<p>At the moment, there is no resource manager/scheduler. Depending on how the resources are used, this may change.</p>"},{"location":"hpc/systems/supported/catscan/#additional-notes","title":"Additional Notes","text":""},{"location":"hpc/systems/supported/catscan/#authentication","title":"Authentication","text":"<p>Currently authentication is LDAP -- the same credentials that you would use to access gmail.lbl.gov. We are in the process of evaluating OTP over ssh keys, or simply OTP.</p>"},{"location":"hpc/systems/supported/catscan/#accessing-the-compute-nodes","title":"Accessing the Compute nodes","text":"<p>You will need to generate ssh-keys for intra-cluster access to the compute nodes. </p> <p>To do this run: </p> <pre><code>ssh-keygen -t ed25519\n</code></pre> <p>Then run </p> <pre><code>cat ~/.ssh/id_ed25519.pub &gt;&gt; ~/.ssh/authorized_keys\n</code></pre> <p>Then finally</p> <pre><code>chmod 600 ~/.ssh/authorized_keys\n</code></pre> <p>The suggested default name and location should be fine. You will be prompted for a password, but to leave it blank just hit enter. Passwords can interfere with intra-cluster node communication when launching jobs, particularly with a scheduler should we choose to deploy one.</p>"},{"location":"hpc/systems/supported/catscan/#storage-for-data","title":"Storage for Data","text":"<p>Each user will be granted space under <code>/clusterfs/bebb/users</code> for data. There is also a group directory to which the group has write permission. This directory should be used for custom builds of software of which the group may want to take advantage. This is modeled after what we offer on our clusters in 1275, but this is your cluster so you can choose to use it how you desire.</p>"},{"location":"hpc/systems/supported/catscan/#software-module-farm","title":"Software Module Farm","text":"<p>See Documentation on using \"modules\".</p>"},{"location":"hpc/systems/supported/catscan/#apptainer-formerly-known-as-singularity","title":"Apptainer (formerly known as Singularity)","text":"<p>Some of you have expressed the need to import custom software built on different architectures. Instead of re-inventing this on the catscan architecture, you can use Apptainer. </p> <p>Apptainer enables users to have full control of their environment. Apptainer can be used to package entire scientific workflows, software and libraries, and even data. </p> <p>To get started, check out these links:</p> <p>Documentation on using Singularity on a cluster called Savio that we manage on UCB cluster </p>"},{"location":"hpc/systems/supported/dirac1/","title":"DIRAC1 Cluster","text":"<p>The DIRAC1 cluster is part of the LBNL Supercluster and shares the same Supercluster infrastructure. This includes the system management software, software module farm, scheduler, storage and backend network management.</p>"},{"location":"hpc/systems/supported/dirac1/#login-and-data-transfer","title":"Login and Data Transfer","text":"<p>DIRAC1 uses One Time Password (OTP) for login authentication for all the services provided below. </p> <ul> <li>Login server: <code>lrc-login.lbl.gov</code></li> <li>DATA transfer server: <code>lrc-xfer.lbl.gov</code></li> <li>Globus Online endpoint: <code>lbnl#lrc</code></li> </ul>"},{"location":"hpc/systems/supported/dirac1/#hardware-configuration","title":"Hardware Configuration","text":"<p>DIRAC1 has a mixture of different type of hardware. Compute nodes are connected with each other through a high-performance Mellanox 56Gbps FDR switch.</p> PARTITION NODES NODE LIST CPU CORES MEMORY dirac1 56 n0[000-055].dirac1 INTEL XEON E5-2670 v3 24 256GB dirac1 72 n0[056-127].dirac1 INTEL XEON E5-2650 v4 24 256GB"},{"location":"hpc/systems/supported/dirac1/#storage-and-backup","title":"Storage and Backup:","text":"<p>DIRAC1 cluster users are entitled to access the following storage systems so please get familiar with them.</p> NAME LOCATION QUOTA BACKUP ALLOCATION DESCRIPTION HOME /global/home/users/$USER 12GB Yes Per User HOME directory for permanent data storage GROUP-SW /global/home/groups-sw/$GROUP 200GB Yes Per Group GROUP directory for software and data sharing with backup GROUP /global/home/groups/$GROUP 400GB No Per Group GROUP directory for data sharing without backup SCRATCH /global/scratch/users/$USER none No Per User SCRATCH directory with Lustre high performance parallel file system CLUSTERFS /clusterfs/dirac1/$USER none No Per User Private storage <p>Note</p> <p>HOME, GROUP, GROUP-SW and CLUSTERFS directories are located on a highly reliable enterprise level BlueArc storage device. Since this appliance also provides storage for many other mission critical file systems, and it is not designed for high performance applications, running large I/O dependent jobs on these file systems could greatly degrade the performance of all the file systems that are hosted on this device and affect hundreds of users, thus this behavior is explicitly prohibited. HPCS reserves the right to kill these jobs without notification once discovered. Jobs that have I/O requirement should use the SCRATCH file system which is designed specifically for that purpose.</p>"},{"location":"hpc/systems/supported/dirac1/#scheduler-configuration","title":"Scheduler Configuration:","text":"<p>DIRAC1 cluster uses SLURM as the scheduler to manage jobs on the cluster. To use the DIRAC1 resource the partition <code>dirac1</code> must be used (<code>--partition=dirac1</code>) along with account <code>dirac1</code> (<code>--account=dirac1</code>). A standard fair-share policy with a decay half life value of 14 days (2 weeks) is enforced. For users from <code>ac_ciftgp</code> (Guggenheim Users), the default QOS is <code>dirac1_normal</code>. This is the high priority QOS which will preempt low priority jobs if those jobs need resources. For users from <code>ac_cifgres</code>, the default QOS is <code>dirac1_lowprio</code>. This is the low priority QOS. When system is busy and there are higher priority jobs pending, scheduler will preempt jobs that are running with this low priority QOS. <code>dirac1_highprio</code> is a special QoS which will preempt both <code>dirac1_lowprio</code> and <code>dirac1_normal</code>.</p> PARTITION ACCOUNT NODES NODE LIST NODE FEATURES SHARED QOS QOS LIMIT dirac1 dirac1 128 n0[000-127].dirac1 dirac1 Exclusive dirac1_lowprio dirac1_normal dirac1_highprio no limit no limit no limit"},{"location":"hpc/systems/supported/dirac1/#software-configuration","title":"Software Configuration","text":"<p>DIRAC1 uses Software Module Farm Environment Modules to manage the cluster wide software installation.</p>"},{"location":"hpc/systems/supported/dirac1/#cluster-status","title":"Cluster Status","text":"<p>Please visit here for the live status of DIRAC1 cluster.</p>"},{"location":"hpc/systems/supported/dirac1/#additional-information","title":"Additional Information","text":"<p>Please send us tickets to hpcshelp@lbl.gov or send email to ScienceIT@lbl.gov for any inquiries or service requests.</p>"},{"location":"hpc/systems/supported/etna/","title":"ETNA Cluster","text":"<p>The ETNA cluster is part of the LBNL Supercluster and shares the same Supercluster infrastructure. This includes the system management software, software module farm, scheduler, storage, and backend network management.</p>"},{"location":"hpc/systems/supported/etna/#login-and-data-transfer","title":"Login and Data Transfer","text":"<p>ETNA uses One Time Password (OTP) for login authentication for all the services provided below. Please also refer to the Data Transfer page for additional information.</p> <ul> <li>Login server: <code>lrc-login.lbl.gov</code></li> <li>DATA transfer server: <code>lrc-xfer.lbl.gov</code></li> <li>Globus Online endpoint: <code>lbnl#lrc</code></li> </ul>"},{"location":"hpc/systems/supported/etna/#hardware-configuration","title":"Hardware Configuration","text":"<p>Each compute node has dual-socket 12-core INTEL Xeon E5-2670 v3 @ 2.30 GHz processors (24 cores in total), 64 GB of physical memory. Compute nodes are connected with each other through a high performance Mellanox 56 Gbps FDR Infiniband fabric.</p> PARTITION NODES CPU CORES MEMORY GPU etna 170 INTEL XEON E5-2670 v3 24 64GB \u2013 etna 3 INTEL XEON E5-2670 v3 24 64GB Xeon Phi etna 16 INTEL XEON E5-2623 v3 8 64GB K80, V100"},{"location":"hpc/systems/supported/etna/#storage-and-backup","title":"Storage and Backup","text":"<p>ETNA cluster users are entitled to access the following storage systems so please get familiar with them.</p> NAME LOCATION QUOTA BACKUP ALLOCATION DESCRIPTION HOME <code>/global/home/users/$USER</code> 12GB Yes Per User HOME directory for permanent data storage SCRATCH <code>/global/scratch/users/$USER</code> none No Per User SCRATCH directory with Lustre high performance parallel file system over Infiniband MOTEL <code>/clusterfs/vulcan/motel/$USER</code> none No Per User Long-term storage of bulk data MOTEL2 <code>/clusterfs/vulcan/motel2/$USER</code> none No Per User Long-term storage of bulk data PSCRATCH <code>/clusterfs/etna/pscratch/$USER</code> none No Per User SCRATCH directory with Lustre high performance parallel file system over Infiniband <p>Note</p> <p><code>HOME</code>, <code>MOTEL</code>, and <code>MOTEL2</code> directories are located on a highly reliable enterprise level BlueArc storage device. Since this appliance also provides storage for many other mission critical file systems, and it is not designed for high performance applications, running large I/O dependent jobs on these file systems could greatly degrade the performance of all the file systems that are hosted on this device and affect hundreds of users, thus this behavior is explicitly prohibited. HPCS reserves the right to kill these jobs without notification once discovered. Jobs that have I/O requirement should use the <code>SCRATCH</code> or <code>PSCRATCH</code> file system which are designed specifically for that purpose.</p>"},{"location":"hpc/systems/supported/etna/#scheduler-configuration","title":"Scheduler Configuration","text":"<p>ETNA cluster uses SLURM as the scheduler to manage jobs on the cluster. To use the ETNA resource the partition <code>etna</code> must be used (<code>\u2013partition=etna</code>). Users of projects <code>nano</code> and <code>etna</code> are allowed to submit jobs to the ETNA cluster using either <code>--account=etna</code> or <code>--account=nano</code>; details on checking your slurm associations are here. For the GPU nodes, use <code>\u2013partition=etna_gpu</code>. Currently there is no special limitation introduced to the <code>etna</code> partition thus no QoS configuration is required to use the ETNA resources (a default <code>normal</code> QoS will be applied automatically). A standard fair-share policy with a decay half life value of 14 days (2 weeks) is enforced.</p> PARTITION NODES NODE LIST NODE FEATURES SHARED etna 175 n0[000-174].etna0 etna, etna_phi([172-174]) Exclusive etna_gpu 16 n0[175-183,238,299-304].etna0 etna_gpu, etna_k80, etna_v100, etna_v100_32g Shared etna-shared 5 n0[188-192].etna0 etna_share Shared etna_c40 16 n0[239-254].etna0 etna_c40 Exclusive etna_bigmem 47 n0[186-187,193-237].etna0 etna_bigmem Exclusive"},{"location":"hpc/systems/supported/etna/#software-configuration","title":"Software Configuration","text":"<p>ETNA uses Software Module Farm to manage the cluster-wide software installation.</p>"},{"location":"hpc/systems/supported/etna/#cluster-status","title":"Cluster Status","text":"<p>Please visit here  for the live status of ETNA cluster.</p>"},{"location":"hpc/systems/supported/mhg/","title":"MHG Cluster","text":"<p>The MHG cluster is part of the LBNL Supercluster and shares the same Supercluster infrastructure. This includes the system management software, software module farm, scheduler, storage, and backend network management.</p>"},{"location":"hpc/systems/supported/mhg/#login-and-data-transfer","title":"Login and Data Transfer","text":"<p>MHG uses One Time Password (OTP) for login authentication for all the services provided below. Please also refer to the Data Transfer page for additional information.</p> <ul> <li>Login server: <code>lrc-login.lbl.gov</code></li> <li>DATA transfer server: <code>lrc-xfer.lbl.gov</code></li> <li>Globus Online endpoint: <code>lbnl#lrc</code></li> </ul>"},{"location":"hpc/systems/supported/mhg/#hardware-configuration","title":"Hardware Configuration","text":"<p>MHG cluster has a mixture of different CPU architectures and memory configurations so please be aware of them and choose them wisely along with the scheduler configurations.</p> PARTITION NODES NODE LIST CPU CORES MEMORY mhg 72 n0[030-036,041-055].mhg0 AMD Opteron 6376 64 256 GB n0[037-040,082,084].mhg0 AMD Opteron 6376 64 512 GB n0[056-081,083,085-101].mhg0 AMD Opteron 6274 64 256 GB"},{"location":"hpc/systems/supported/mhg/#storage-and-backup","title":"Storage and Backup","text":"<p>MHG cluster users are entitled to access the following storage systems so please get familiar with them.</p> NAME LOCATION QUOTA BACKUP ALLOCATION DESCRIPTION HOME <code>/global/home/users/$USER</code> 12GB Yes Per User HOME directory for permanent data storage GROUP-SW <code>/global/home/groups-sw/$GROUP</code> 200GB Yes Per Group GROUP directory for software and data sharing with backup GROUP <code>/global/home/groups/$GROUP</code> 400GB No Per Group GROUP directory for data sharing without backup SCRATCH <code>/global/scratch/users/$USER</code> none No Per User SCRATCH directory with Lustre high performance parallel file system CLUSTERFS <code>/clusterfs/mhg/$USER</code> none No Per User Private storage LOCAL <code>/local/scratch/users/$USER</code> none No Per User Local scratch on each node <p>Note</p> <p>HOME, GROUP, and GROUP-SW directories are located on a highly reliable enterprise level BlueArc storage device. Since this appliance also provides storage for many other mission critical file systems, and it is not designed for high performance applications, running large I/O dependent jobs on these file systems could greatly degrade the performance of all the file systems that are hosted on this device and affect hundreds of users, thus this behavior is explicitly prohibited. HPCS reserves the right to kill these jobs without notification once discovered. Jobs that have I/O requirement should use the SCRATCH file system which is designed specifically for that purpose.</p>"},{"location":"hpc/systems/supported/mhg/#scheduler-configuration","title":"Scheduler Configuration:","text":"<p>MHG cluster uses SLURM as the scheduler to manage jobs on the cluster. To use the MHG resource the partition <code>mhg</code> must be used (<code>--partition=mhg</code>) along with account <code>mhg</code> (<code>--account=mhg</code>). Currently there is no special limitation introduced to the <code>mhg</code> partition thus no QoS configuration is required to use the MHG resources (a default <code>normal</code> QoS will be applied automatically). A standard fair-share policy with a decay half life value of 14 days (2 weeks) is enforced. If node feature (<code>--constraint</code> option) is not used, the default dispatch order will be: <code>mhg_c4, mhg_c8, mhg_c32, mhg_c48, mhg_m256, mhg_m512</code>.</p> PARTITION ACCOUNT NODES NODE LIST NODE FEATURES SHARED QOS QOS LIMIT mhg mhg 72 n0[030-036].mhg0 n0[037-039].mhg0 n0040.mhg0 n0[041-055].mhg0 n0[056-101].mhg0 mhg, mhg_c64, mhg_m256 mhg, mhg_c64, mhg_m512 mhg, mhg_c64, mhg_m512, mhg_ssd mhg, mhg_c64, mhg_m256 mhg, mhg_c64, mhg_m256 Yes normal no limit"},{"location":"hpc/systems/supported/mhg/#software-configuration","title":"Software Configuration","text":"<p>ETNA uses Software Module Farm to manage the cluster-wide software installation.</p>"},{"location":"hpc/systems/supported/mhg/#cluster-status","title":"Cluster Status","text":"<p>Please visit here  for the live status of MHG cluster.</p>"},{"location":"hpc/systems/supported/mhg/#additional-information","title":"Additional Information:","text":"<p>Please send us tickets hpcshelp@lbl.gov or send email to ScienceIT@lbl.gov for any inquiries or service requests.</p>"},{"location":"snippets/help/","title":"Help","text":""},{"location":"snippets/help/#hpc-helpdesk","title":"HPC Helpdesk","text":"<p>Contact us for help with debugging jobs on our HPC clusters, software installation, user account management and more.</p> <ul> <li>HPC Email Support: hpcshelp@lbl.gov (Creates a ticket in AskUS)</li> <li>AskUS Request Form: HPC Help Request</li> <li>Office Hours: HPC &amp; Science IT Office Hours: Every Wednesday 10:30-Noon (Zoom)</li> </ul>"},{"location":"snippets/help/#science-it-consulting","title":"Science IT Consulting","text":"<p>Science IT Consultants bring broad expertise in technology deployment, high performance computing and cloud infrastructure. Contact us to discuss your needs in scientific computing, research data management, cloud computing, AI/ML workflows, and more.</p> <ul> <li>Schedule a Science IT Consulting Engagement</li> <li>Email Science IT: scienceit@lbl.gov </li> <li>Office Hours: HPC &amp; Science IT Office Hours: Every Wednesday 10:30-Noon (Zoom)</li> </ul>"},{"location":"tutorials/jupyter-gpu/","title":"Using Jupyter on GPU","text":"In\u00a0[10]: Copied! <pre>import sys\nimport numpy as np\nimport tensorflow as tf\n</pre> import sys import numpy as np import tensorflow as tf In\u00a0[11]: Copied! <pre>print (sys.version)\n</pre> print (sys.version) <pre>3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]\n</pre> In\u00a0[12]: Copied! <pre>print (tf.__version__)\n</pre> print (tf.__version__) <pre>2.15.0\n</pre> In\u00a0[13]: Copied! <pre>tf.config.list_physical_devices('GPU')\n</pre> tf.config.list_physical_devices('GPU') Out[13]: <pre>[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]</pre> In\u00a0[14]: Copied! <pre>device_name = tf.test.gpu_device_name()\nprint (device_name)\n</pre> device_name = tf.test.gpu_device_name() print (device_name) <pre>/device:GPU:0\n</pre> <pre>2024-02-08 09:33:51.770011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /device:GPU:0 with 9776 MB memory:  -&gt; device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:3b:00.0, compute capability: 7.5\n</pre> In\u00a0[15]: Copied! <pre>mnist = tf.keras.datasets.mnist\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\n</pre> mnist = tf.keras.datasets.mnist (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train, x_test = x_train / 255.0, x_test / 255.0 In\u00a0[16]: Copied! <pre>model = tf.keras.models.Sequential([\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\n  tf.keras.layers.Dense(128, activation='relu'),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(10)\n])\n\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmodel.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n</pre> model = tf.keras.models.Sequential([   tf.keras.layers.Flatten(input_shape=(28, 28)),   tf.keras.layers.Dense(128, activation='relu'),   tf.keras.layers.Dropout(0.2),   tf.keras.layers.Dense(10) ])  loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy']) In\u00a0[17]: Copied! <pre>model.fit(x_train, y_train, epochs=5)\n</pre> model.fit(x_train, y_train, epochs=5) <pre>Epoch 1/5\n1875/1875 [==============================] - 11s 5ms/step - loss: 0.2947 - accuracy: 0.9147\nEpoch 2/5\n1875/1875 [==============================] - 10s 5ms/step - loss: 0.1429 - accuracy: 0.9578\nEpoch 3/5\n1875/1875 [==============================] - 10s 5ms/step - loss: 0.1066 - accuracy: 0.9678\nEpoch 4/5\n1875/1875 [==============================] - 10s 5ms/step - loss: 0.0890 - accuracy: 0.9725\nEpoch 5/5\n1875/1875 [==============================] - 10s 5ms/step - loss: 0.0748 - accuracy: 0.9766\n</pre> Out[17]: <pre>&lt;keras.src.callbacks.History at 0x2b2e31914580&gt;</pre> In\u00a0[18]: Copied! <pre>model.evaluate(x_test, y_test, verbose=2)\n</pre> model.evaluate(x_test, y_test, verbose=2) <pre>313/313 - 1s - loss: 0.0775 - accuracy: 0.9751 - 1s/epoch - 4ms/step\n</pre> Out[18]: <pre>[0.07746326178312302, 0.9750999808311462]</pre>"},{"location":"tutorials/jupyter-gpu/#using-jupyter-on-gpu","title":"Using Jupyter on GPU\u00b6","text":"<ul> <li>If the import of tensorflow fails, then make sure that you have chosen the appropriate jupyter kernel.</li> </ul>"},{"location":"tutorials/jupyter-gpu/#import-modules-and-check-versions","title":"Import modules and check versions\u00b6","text":""},{"location":"tutorials/jupyter-gpu/#test-if-tensorflow-can-access-a-gpu","title":"Test if Tensorflow can access a GPU\u00b6","text":""},{"location":"tutorials/jupyter-gpu/#run-a-simple-example-from-tensorflow-tutorials","title":"Run a simple example from Tensorflow tutorials\u00b6","text":"<p>The beginner tutorial can be found at: https://www.tensorflow.org/tutorials/quickstart/beginner</p>"},{"location":"tutorials/jupyter-gpu/#load-a-dataset","title":"Load a dataset\u00b6","text":""},{"location":"tutorials/jupyter-gpu/#build-a-machine-learning-model","title":"Build a machine learning model\u00b6","text":""},{"location":"tutorials/jupyter-gpu/#train-your-model","title":"Train your model\u00b6","text":""},{"location":"tutorials/jupyter-gpu/#evaluate-your-model","title":"Evaluate your model\u00b6","text":""}]}